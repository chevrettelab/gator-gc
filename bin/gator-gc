#!/bin/env python

import time
import sys
import csv
import os
import glob
import logging
import pandas as pd
import numpy as np
import subprocess
import tempfile
import argparse
import seaborn as sns
import multiprocessing
import gator_gc.flat
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch

from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio.SeqFeature import SeqFeature
from Bio.SeqFeature import CompoundLocation
from tempfile import TemporaryDirectory
from operator import itemgetter
from collections import defaultdict
from pygenomeviz import GenomeViz
from matplotlib.patches import Patch
from scipy.spatial.distance import pdist
from scipy.stats import norm
from scipy.cluster.hierarchy import dendrogram, linkage
from typing import List, Set, Dict, Tuple, Union, Optional, Any, TextIO

##testing this for MacOS
if sys.platform == "darwin":  # MacOS
    multiprocessing.set_start_method("spawn", force=True)
elif sys.platform == "linux":
    multiprocessing.set_start_method("fork", force=True)

## Constants
NRPS = set(['NRPS-C', 'NRPS-A'])

PKS = set(['PKS-KS', 'PKS-AT'])

MODULE_TYPE = {
    'Condensation_LCL': 'NRPS-C',
    'Condensation_DCL': 'NRPS-C',
    'Condensation_Dual': 'NRPS-C',
    'Condensation_Starter': 'NRPS-C',
    'Cglyc': 'NRPS-C',
    'AMP-binding': 'NRPS-A',
    'A-OX': 'NRPS-A',
    'Enediyne-KS': 'PKS-KS',
    'Modular-KS': 'PKS-KS',
    'Trans-AT-KS': 'PKS-KS',
    'Hybrid-KS': 'PKS-KS',
    'PKS_KS': 'PKS-KS',
    'Iterative-KS': 'PKS-KS',
    'PKS_AT': 'PKS-AT'
}

MODULAR_DOMAINS_HMMDB = os.path.join(os.path.dirname(gator_gc.flat.__file__),'modular_domains.hmmdb')
GENBANK_EXTENSIONS = [
    '*.gbk',
    '*.gbff',
    '*.gb'
]
REQUIRED_DISTANCE_FACTOR = 1000
WINDOW_EXTENSION_FACTOR = 1000
EDGE_THRESHOLD = 500
VERSION = 'v1.0.0'
DESCRIPTION = """

     -\ ---\--\ -------\ ----\ ---\--\ ---\ --\ ----\ ----\--------\ /--- 
   /--/ ---/--/ -------/ ----/ ---/--/ ---/ --/ ----/ ----/--------/ \----\ 
  ________   _____  __________________  __________         _________________  
 /  _____/  /  _  \ \__    ___/_____  \ \______   \       /  _____/\_   ___ \ 
/   \  ___ /  /_\  \  |    |   /   |   \ |       _/ _____    \  ___/    \  \/ 
\    \_\  \    |    \ |    |  /    |    \|    |   \/_____/    \_\  \     \_____
 \______  /____|__  / |____|  \_______  /|____|_  /       \______  /\_______  /
        \/        \/                  \/        \/               \/         \/ 
    -----\--\ -------\ ----\ ---\--\ ---\ --\ ------\ ----\---------\ /----
    \----/--/ -------/ ----/ ---/--/ ---/ --/ ------/ ----/---------/ \----/

GATOR-GC: Genomic Assessment Tool for Orthologous Regions and Gene Clusters
Developer: José D. D. Cediel-Becerra
Afiliation: Microbiology & Cell Science Deparment, University of Florida
Please contact José at jcedielbecerra@ufl.edu if you have any issues
Version:"""+VERSION

def parse_gator_arguments():
    parser = argparse.ArgumentParser(description=DESCRIPTION, formatter_class=argparse.RawTextHelpFormatter)

    input_group = parser.add_argument_group("Input Options")
    input_group.add_argument(
        '-rq',
        type=str,
        help='Path to the query protein FASTA file containing required proteins.',
        metavar='',
        required=True
    )
    input_group.add_argument(
        '-op',
        type=str,
        help='Path to the query protein FASTA file containing optional proteins.',
        metavar='',
        required=False
    )
    input_group.add_argument(
        '-g',
        type=str,
        nargs='+',
        help='Directory containing the Genbank files (*.gbff/*.gbk/*.gb).'
             'You can specify multiple directories separated by spaces. '
             'Directories can be specified with or without wildcards.',
        metavar='',
        required=True
    )
    input_group.add_argument(
        '-d',
        type=str,
        help='Directory containing the PRE-GATOR-GC databases (.dmnd and .domtblout files). (Required)',
        metavar='',
        required=True
    )

    alignment_group = parser.add_argument_group("Diamond Options")
    alignment_group.add_argument(
        '-t',
        type=int,
        default=multiprocessing.cpu_count(),
        help='Number of CPUs to use for diamond search and hmmsearch. (Default: all available CPUs)',
        metavar='',
        required=False
    )
    alignment_group.add_argument(
        '-us',
        action='store_true',
        default=False,
        help='Enable ultra-sensitive DIAMOND alignment. This overrides query cover and protein percent values only for the window search.(Default: False)',
        required=False
    )
    alignment_group.add_argument(
        '-qc',
        type=float,
        default=70,
        help='Minimum percent query cover for diamond search (Default: 70). if ultra sensitive was enabled, query cover value is used only for all vs all comparisons',
        metavar='',
        required=False
    )
    alignment_group.add_argument(
        '-idt',
        type=float,
        default=35,
        help='Minimum percent identity for diamond search. (Default: 35). if ultra sensitive was enabled, percent identity value is used only for all vs all comparisons',
        metavar='',
        required=False
    )
    alignment_group.add_argument(
        '-bs',
        type=float,
        default=2,
        help='DIAMOND block size. It controlls DIAMOND memory and disk space. For large datasets, increase this but keep an eye on memory and disk (Default: 2)',
        metavar='',
        required=False
    )
    alignment_group.add_argument(
        '-k',
        type=int,
        default=int(1e50),
        help='Number of target sequences. (Default: 1e50)',
        metavar='',
        required=False
    )
    hmmer_group = parser.add_argument_group("HMMER Options")
    hmmer_group.add_argument(
        '-e',
        type=float,
        default=1e-4,
        help='E-value threshold for hmmsearch. (Default: 1e-4)',
        metavar='',
        required=False
    )

    gator_group = parser.add_argument_group("GATOR-GC Options")
    gator_group.add_argument(
        '-rd',
        type=float,
        default=86,
        help='Maximum distance in kilobases between required genes to define a gator window. (Default: 86 kb for bacteria, 174 kb for fungi, 262 kb for plants)',
        metavar='',
        required=False
    )
    gator_group.add_argument(
        '-we',
        type=int,
        default=10,
        help='Extension in kilobases from the start and end positions of the gator windows. (Default: 10 kb)',
        metavar='',
        required=False
    )

    out_group = parser.add_argument_group("Output Options")
    out_group.add_argument(
        '-o',
        type=str,
        help='Directory to save GATOR-GC results.',
        metavar='',
        required=True
    )
    out_group.add_argument(
        '-nc',
        action='store_true',
        help='Disable creation of GATOR conservation figures.',
        required=False
    )
    out_group.add_argument(
        '-nn',
        action='store_true',
        help='Disable creation of GATOR neighborhoods figures.',
        required=False
    )

    parser.add_argument(
        '-v',
        action='store_true',
        default=False,
        help='Enable verbose output. (Default: False)',
        required=False
    )

    return parser.parse_args()

def elapsed_time(
        stime: float
) -> None:
    """
    Time used to process GATOR-GC.

    Args:
        stime (float): Start time of the process.

    Returns:
        None
    """
    etime = time.time()
    elapsed_time = etime - stime

    if elapsed_time < 60:
        time_unit = "seconds"
        ftime = round(elapsed_time, 2)
    elif elapsed_time < 3600:
        time_unit = "minutes"
        ftime = round(elapsed_time / 60, 2)
    else:
        time_unit = "hours"
        ftime = round(elapsed_time / 3600, 2)
    logging.info(f"Execution time: {ftime} {time_unit}")

stime = time.time()
np.random.seed(53000)
plt.rcParams['font.family'] = 'Arial'

class CustomFormatter(logging.Formatter):
    def __init__(self, fmt=None, datefmt=None):
        super().__init__(fmt, datefmt)
        self.counter = 0

    def format(self, record):
        self.counter += 1
        record.custom_counter = f"[{self.counter}]"
        return super().format(record)

log_format = '%(custom_counter)s - %(asctime)s - %(levelname)s - %(message)s'
formatter = CustomFormatter(log_format)
handler = logging.StreamHandler()
handler.setFormatter(formatter)
logging.basicConfig(level=logging.INFO, handlers=[handler])


class NoRequiredProteinsGroupedContigsError(Exception):
    """Custom exception to be raised when no required proteins are organized into contigs"""
    pass

class NoValidWindowsError(Exception):
    """Custom exception to be raised when no valid windows are found."""
    pass

class GenomeProcessingError(Exception):
    """Custom exception for errors during genome processing."""
    pass

def create_directory(
        directory_name: str,
        verbose: bool = False
) -> None:
    """
    Create a directory to save the GATOR-GC output.

    Arguments:
    directory_name (str): Name of the directory to create.
    verbose (bool): Flag to control verbose logging.

    Raises:
    ValueError: If the directory name contains invalid characters or is empty.
    FileExistsError: If the directory already exists.
    OSError: If the directory creation fails due to OS-level error.
    """
    if not directory_name:
        logging.error("ERROR: Directory name cannot be empty.")
        raise ValueError("Directory name cannot be empty")

    if '--' in directory_name or any(char in directory_name for char in '<>:"\\|?*'):
        logging.error(f"ERROR: Directory name {directory_name} contains invalid characters.")
        raise ValueError("Invalid directory name")

    if os.path.exists(directory_name):
        logging.error(f"ERROR: Directory {directory_name} already exists")
        raise FileExistsError("Directory already exists")

    try:
        os.mkdir(directory_name)
        if verbose:
            logging.info(f"The {directory_name} directory was created successfully.")
    except OSError as e:
        logging.error(f"ERROR: Failed to create the {directory_name} directory due to OS-level error: {e}")
        raise

def get_list_genomes(
        genomes_dirs: List[str],
        verbose: bool = False
) -> List[str]:
    """                                                                                                                              

    Retrieves a list of genome file paths based on provided directory paths and optional wildcard patterns.                          
    It handles multiple directory inputs and supports wildcard patterns for file matching.                                           

    Args:                                                                                                                            
    genomes_dirs (List[str]): A list containing directory paths which may include wildcards to specify files.                        
    verbose (bool): Flag to enable logging of detailed execution messages.                                                           
    Returns:                                                                                                                         
    List[str]: A list containing the full paths to genome files matching the input patterns. If a directorydoes not contain a wildcard, all genome files in the directory are included based on predefined extensions.                         
    """
    genomes_list = []
    for directory in genomes_dirs:
        dir_path, wildcard_pattern = os.path.split(directory)
        if not dir_path:
            dir_path = '.'
        expanded_dirs = glob.glob(dir_path)
        if not expanded_dirs:
            logging.warning(f"No directories found matching: {dir_path}")
        if not wildcard_pattern:
            for expanded_dir in expanded_dirs:
                found_files = False
                for genome_extension in GENBANK_EXTENSIONS:
                    matched_files = glob.glob(os.path.join(expanded_dir, genome_extension))
                    genomes_list.extend(matched_files)
                    if matched_files:
                        found_files = True
                if not found_files:
                    logging.warning(f"No genome files found in: {expanded_dir} with predefined extensions {GENBANK_EXTENSIONS}")       
        else:   
            for expanded_dir in expanded_dirs:
                matched_files = glob.glob(os.path.join(expanded_dir, wildcard_pattern))
                genomes_list.extend(matched_files)
                if not matched_files:
                    logging.warning(f"No files found matching pattern {wildcard_pattern} in: {expanded_dir}")
                elif verbose:
                    logging.debug(f"Found files matching pattern {wildcard_pattern} in: {expanded_dir}")
    if not genomes_list:
        logging.error("No genome files found matching the provided patterns and directories.")
    if verbose:
        logging.info(f"Total genome files found: {len(genomes_list)}")

    return genomes_list


def init_modular(
        protein_dict: Dict[str, Dict[str, bool]]
) -> Dict[str, Dict[str, bool]]:
    """
    Create a dictionary structure for query proteins and make subkeys for boolean decisions (is_nrps, is_pks).
    
    Arguments:
    protein_dict (Dict): Dictionary containing as keys the headers for required and optional proteins and subkeys for boolean decision (is_nrps, is_pks).
    
    Returns:
    protein_dict (Dict): Same protein_dict dictionary structure with updated boolean fields.
    """
    for header in protein_dict:
        protein_dict[header]['is_nrps'] = False
        protein_dict[header]['is_pks'] = False
        protein_dict[header]['hit'] = set()
    return protein_dict

def parse_query_faa(
        query_faa: str,
        query_type: str,
        protein_dict: Dict[str, Dict[str, str]]
) -> Dict[str, Dict[str, str]]:
    """
    Adds two more subkeys in the protein_dict dictionary for each protein in a FASTA file: query_type and seq.
    
    Arguments:
    query_faa (str): File name of the query protein FASTA file.
    query_type (str): Type of the query protein (i.e., required or optional).
    protein_dict (Dict): Dictionary containing headers for proteins with subkeys for various details.
    
    Returns:
    Dict: Updated protein_dict with added subkeys after calling init_modular function.
    """
    try:
        with open(query_faa, 'r') as ifh:
            header, sequence = None, ''
            for line in ifh.read().splitlines():
                if line.startswith('>'):
                    if header:
                        header = header.strip().replace(' ', '_')
                        if header not in protein_dict:
                            protein_dict[header] = {'query_type': query_type, 'seq': ''}
                        protein_dict[header]['seq'] = sequence
                    header, sequence = line[1:], ''
                else:
                    sequence += line
            if header:
                header = header.strip().replace(' ', '_')
                if header not in protein_dict:
                    protein_dict[header] = {'query_type': query_type, 'seq': ''}
                protein_dict[header]['seq'] = sequence
    except FileNotFoundError:
        raise FileNotFoundError(f"The file {query_faa} was not found.")
    except Exception as e:
        raise RuntimeError(f"Failed to parse the file {query_faa}: {str(e)}")
    return init_modular(protein_dict)

def run_hmmsearch(
        hmmdb: str,
        query: str,
        out: str,
        cpu: int,
        e_value: float,
        verbose: bool = False
) -> None:
    """
    Runs the hmmsearch pipeline to search for modular domain hits in proteins.

    Arguments:
    hmmdb (str): Path to the HMMs modular domains (nrps, pks) database.
    query (str): Path to the query protein file.
    out (str): Output file name for the modular domtblout table.
    cpu (int): Number of CPUs allocated for running hmmsearch.
    e_value (float): E-value threshold for hmmsearch.
    verbose (bool): Flag to control verbose logging.                                                                                                                                             

    Returns:
    None: This function does not return anything, it executes hmmsearch and writes output to a file.

    Raises:
    ValueError: If any input argument is invalid.
    RuntimeError: If the hmmsearch command fails.
    """
    if not os.path.isfile(hmmdb):
        raise ValueError(f"HMM database file {hmmdb} does not exist.")
    if not os.path.isfile(query):
        raise ValueError(f"Query protein file {query} does not exist.")
    command = [
        'hmmsearch',
        '--domtblout', out,
        '--cpu', str(cpu),
        '--domE', str(e_value),
        '--noali',
        '-o', '/dev/null',
        hmmdb,
        query
    ]
    try:
        subprocess.run(command, check=True)
        if verbose:
            logging.info(f"hmmsearch completed successfully. Output written to {out}")
    except subprocess.CalledProcessError as e:
        logging.error(f"hmmsearch failed with error: {e}")
        raise RuntimeError(f"hmmsearch failed: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}")
        raise RuntimeError(f"An unexpected error occurred: {e}")

def parse_modular_domtblout(
        domtbl: str,
        verbose: bool = False
) -> Dict[str, Set[str]]:
    """
    Parses the modular domtblout table and categorizes the domain hits according to the MODULE_TYPE constant.

    Arguments:
    domtbl (str): Path to the modular domtblout table.

    Returns:
    Dict[str, Set[str]]: A dictionary where each key is a query protein and the value is a set of MODULE_TYPEs for each hit found.

    Raises:
    FileNotFoundError: If the file is not found.
    Exception: If an error occurs during parsing.
    """
    modular_domain_hit = defaultdict(set)
    try:
        with open(domtbl, 'r') as dfh:
            for line in dfh:
                if line.startswith('#'):
                    continue
                fields = line.split()
                if len(fields) > 3:
                    query_protein, domain_hit = fields[0], fields[3]
                    if domain_hit in MODULE_TYPE:
                        modular_domain_hit[query_protein].add(MODULE_TYPE[domain_hit])
                    else:
                        logging.warning(f"Unknown domain hit: {domain_hit} in protein {query_protein}.")
                        raise ValueError(f"Unknown domain hit: {domain_hit}")
    except FileNotFoundError:
        logging.error(f"The file {domtbl} was not found.")
        raise FileNotFoundError(f"The file {domtbl} was not found.")
    except Exception as e:
        logging.error(f"An error occurred while parsing the file: {str(e)}")
        raise Exception(f"An error occurred while parsing the file: {str(e)}")
    if verbose:
        logging.info(f"Parsing modular domtblout completed successfully. Found {len(modular_domain_hit)} proteins with domain hits.")
    return modular_domain_hit

def parse_modular_domain_query_hits(
        modular_domain_hit: Dict[str, Set[str]],
        protein_dict: Dict[str, Dict[str, bool]],
        verbose: bool = False
) -> Dict[str, Dict[str, bool]]:
    """                                                                                                                                                          
    Updates protein_dict with boolean decisions based on MODULE_TYPE categorizations from modular_domain_hit.                                         
                                                                                                                                   
    Arguments:                                                                                                                                                   

    modular_domain_hit (Dict): Dictionary containing the MODULE_TYPE categorization for each hit found.                                                          

    protein_dict (Dict): Dictionary structure with keys as protein headers and subkeys for is_nrps and is_pks initialized as False.                              

    Returns:                                                                                                                                                     
    Dict: Updated protein_dict with boolean values set for is_nrps and is_pks based on presence of NRPS and PKS domain types.                                    
    """
    if not modular_domain_hit:
        if verbose:
            logging.info("User query proteins do not have any hit for modular domains.")
        return protein_dict
    try:
        for query_protein in modular_domain_hit:
            #if query_protein not in protein_dict:
                #logging.warning(f"Query protein {query_protein} not found in protein_dict. Adding it.")
                #protein_dict[query_protein] = {'is_nrps': False, 'is_pks': False}
            if NRPS.issubset(modular_domain_hit[query_protein]):
                protein_dict[query_protein]['is_nrps'] = True
                if verbose:
                    logging.info(f"User query protein {query_protein} marked as NRPS.")
            if PKS.issubset(modular_domain_hit[query_protein]):
                protein_dict[query_protein]['is_pks'] = True
                if verbose:
                    logging.info(f"User query protein {query_protein} marked as PKS.")
    except Exception as e:
        logging.error(f"An error occurred while updating protein_dict: {str(e)}")
        raise

    if verbose:
        logging.info("Protein dictionary  with boolean values set for is_nrps and is_pks completed successfully.")
    return protein_dict

def get_dmnd_domtblout_paths(
        gator_databases: str,
        verbose: bool = False
) -> Tuple[str, str]:
    """
    Retrieves the paths for the Gator Diamond Database and the Gator Domtblout Database within a specified directory.

    Arguments:
    gator_databases (str): Path to the folder containing Gator databases.

    Returns:
    tuple: A tuple containing the paths to the Diamond Database file and the Domtblout Database file.

    Raises:
    FileNotFoundError: If no matching files are found for the required database formats.
    """
    if not os.path.isdir(gator_databases):
        logging.error(f"The directory {gator_databases} does not exist.")
        raise FileNotFoundError(f"The directory {gator_databases} does not exist.")

    dmnd_files = glob.glob(os.path.join(gator_databases, '*.dmnd'))
    if not dmnd_files:
        logging.error("No Diamond database files ('*.dmnd') found in the specified directory.")
        raise FileNotFoundError("No Diamond database files ('*.dmnd') found in the specified directory.")
    dmnd_database = dmnd_files[0]
    if verbose:
        logging.info(f"Found Diamond database file: {dmnd_database}")

    domtblout_files = glob.glob(os.path.join(gator_databases, '*.domtblout'))
    if not domtblout_files:
        logging.error("No Domtblout database files ('*.domtblout') found in the specified directory.")
        raise FileNotFoundError("No Domtblout database files ('*.domtblout') found in the specified directory.")
    domtblout_database = domtblout_files[0]
    if verbose:
        logging.info(f"Found Domtblout database file: {domtblout_database}")

    return dmnd_database, domtblout_database


def parse_modular_domain_genome_hits(
        modular_domain_hit: Dict[str, Set[str]],
        protein_dict: Dict[str, Dict[str, Set[str]]],
        verbose: bool = False
) -> Dict[str, Dict[str, Set[str]]]:
    """
    Integrates modular domain hits from a genome-wide analysis into protein_dict, under the subkey 'hit'.

    Args:
        modular_domain_hit (Dict[str, Set[str]]): Dictionary with protein headers as keys and sets of MODULE_TYPE categorizations as values.
        protein_dict (Dict[str, Dict[str, Set[str]]]): Dictionary with protein headers as keys and details including boolean decisions is_nrps and is_pks.

    Returns:
        Dict[str, Dict[str, Set[str]]]: Updated protein_dict with a new subkey 'hit' populated based on the domain hits.
    """
    try:
        mhs_nrps, mhs_pks, mhs_hybrid = set(), set(), set()
        for genome_protein in modular_domain_hit:
            domains = modular_domain_hit[genome_protein]
            if NRPS.issubset(domains) and PKS.issubset(domains):
                mhs_hybrid.add(genome_protein)
            if NRPS.issubset(domains):
                mhs_nrps.add(genome_protein)
            if PKS.issubset(domains):
                mhs_pks.add(genome_protein)
                
        modular_hit_dict = {'mhs_nrps': mhs_nrps, 'mhs_pks': mhs_pks, 'mhs_hybrid': mhs_hybrid}
        nrps_integrated, pks_integrated, hybrid_integrated = False, False, False
        for query_protein in protein_dict:
            #if 'hit' not in protein_dict[query_protein]:
                #protein_dict[query_protein]['hit'] = set()
            #if 'is_nrps' in protein_dict[query_protein] and 'is_pks' in protein_dict[query_protein]:
            if protein_dict[query_protein]['is_nrps'] and protein_dict[query_protein]['is_pks']:
                protein_dict[query_protein]['hit'].update(modular_hit_dict['mhs_hybrid'])
                hybrid_integrated = True
            elif protein_dict[query_protein]['is_nrps']:
                protein_dict[query_protein]['hit'].update(modular_hit_dict['mhs_nrps'])
                nrps_integrated = True
            elif protein_dict[query_protein]['is_pks']:
                protein_dict[query_protein]['hit'].update(modular_hit_dict['mhs_pks'])
                pks_integrated = True
        if verbose:
            if hybrid_integrated:
                logging.info("Successfully integrated Hybrid hits into protein_dict.")
            if nrps_integrated:
                logging.info("Successfully integrated NRPS hits into protein_dict.")
            if pks_integrated:
                logging.info("Successfully integrated PKS hits into protein_dict.")
            if not (hybrid_integrated or nrps_integrated or pks_integrated):
                logging.info("No need to integrate Hybrids, NRPS, and PKS hits into protein_dict.")

        return protein_dict

    except Exception as e:
        logging.error(f"Error integrating NRPS and PKS hits into protein_dict: {e}")
        raise RuntimeError(f"Failed to integrate NRPS and PKS hits into protein_dict. Error: {e}")

def run_diamond(
    query: str,
    dmnddb: str,
    dmnd_out: str,
    threads: int,
    query_cover: float,
    identity: float,
    ultra_sensitive: bool = False,
    block_size: float = 2.0,
    max_matches: int = 1e50,
    verbose: bool = False
) -> None:
    """
    Runs the DIAMOND BLASTP search.

    Args:
        query (str): Path to the query file.
        dmnddb (str): Path to the DIAMOND database.
        dmnd_out (str): Path to the output file.
        threads (int): Number of threads to use.
        query_cover (float): Minimum query cover percentage.
        identity (float): Minimum identity percentage.
        max_matches (Optional[int]): Maximum number of matches to report per query (default: 1e50).
        verbose (bool): Flag to enable logging of detailed execution messages.
        ADD ULTRA_SENSITIVE AND BLOCK_SIZE ONCE IT IS CONFIRMED TO WORK.      
    Returns:
        None: This function runs the DIAMOND BLASTP command and does not return any value.
    """
    command = [
        'diamond', 'blastp',
        '-q', query,
        '-d', dmnddb,
        '-o', dmnd_out,
        '--quiet',
        '--threads', str(threads),
        '-k', str(max_matches),
        '-b',str(block_size)
    ]
    if not ultra_sensitive:
        command.extend([
            '--query-cover', str(query_cover),
            '--id', str(identity)
        ])
    else:
        command.append('--ultra-sensitive')
    try:
        if verbose:
            logging.debug(f"Running DIAMOND with command: {' '.join(command)}")
        subprocess.run(command, check=True)
        if verbose:
            logging.debug(f"DIAMOND search completed successfully. Output written to {dmnd_out}")

    except subprocess.CalledProcessError as e:
        logging.error(f"DIAMOND search failed with error: {e}")
        raise RuntimeError(f"DIAMOND search failed. Error: {e}")
    except Exception as e:
        logging.error(f"Unexpected error occurred during DIAMOND search: {e}")
        raise RuntimeError(f"Unexpected error occurred during DIAMOND search. Error: {e}")

def parse_diamond_search(
    dmnd: str,
    protein_dict: Dict[str, Dict[str, Set[str]]],
    verbose: bool = False
) -> Dict[str, Dict[str, Set[str]]]:
    """
    Adds the diamond hits to the protein_dict dictionary under the pre-existing 'hit' subkey.

    Args:
        dmnd (str): Path to the diamond table output file.
        protein_dict (Dict[str, Dict[str, Set[str]]]): Dictionary where each protein identifier keys into another dictionary
        that includes a 'hit' key already initialized with a set.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Returns:
        Dict[str, Dict[str, Set[str]]]: Updated protein_dict with diamond hits added to the 'hit' subkey for relevant proteins.

    Raises:
        FileNotFoundError: If the diamond output file does not exist.
    """
    try:
        with open(dmnd, 'r') as mfh:
            is_empty = True
            for line in mfh:
                if line.strip():
                    is_empty = False
                    columns = line.split()
                    if len(columns) >= 2:
                        query_protein, dmnd_hit = columns[0], columns[1]
                        #if query_protein in protein_dict:
                        protein_dict[query_protein]['hit'].add(dmnd_hit)
        if verbose and is_empty:
            raise ValueError(f"The file {dmnd} is empty. No DIAMOND hits found")
        if verbose:
            logging.info(f"Successfully parsed diamond hits from {dmnd} and integrated to protein_dict")

    except FileNotFoundError:
        logging.error(f"Unable to find or open the file {dmnd}")
        raise FileNotFoundError(f"Unable to find or open the file {dmnd}")
    except Exception as e:
        logging.error(f"An error occurred while parsing the file {dmnd}: {str(e)}")
        raise Exception(f"An error occurred while parsing the file {dmnd}: {str(e)}")
    
    return protein_dict

def grouping_user_req_opt_proteins_by_contig(
    protein_dict: Dict[str, Dict],
    query_type: str,
    verbose: bool = False
) -> Tuple[Dict[str, List[Dict]], Set[str]]:
    """
    Groups proteins by the same contig ID based on the specified query_type ('req' or 'opt').
    Adds each hit to a new entry in the output dictionary with additional metadata.

    Args:
        protein_dict (Dict[str, Dict]): Dictionary where keys are protein identifiers and values are dictionaries with protein metadata.
        query_type (str): 'req' or 'opt' indicating if the proteins are required or optional.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Returns:
        Tuple[Dict[str, List[Dict]], Set[str]]:
            - hits_by_contig: Dictionary grouping hits by contig ID.
            - all_required_proteins: Set of all proteins matching the specified query_type.

    Note: Hit entries are expected to be formatted with multiple delimiters ('|-|' and '|_|') for parsing metadata.
    """
    hits_by_contig = {}
    all_required_proteins = set()    
    for query_protein in protein_dict:
        metadata = protein_dict[query_protein]
        if metadata['query_type'] == query_type:
            all_required_proteins.add(query_protein)
            for hit in metadata['hit']:
                split_hit = hit.split('|-|')
                locus, start_position, end_position = split_hit[0], split_hit[1], split_hit[2]
                genome_contig_id = split_hit[0].split('|_|')[1] + '|-|' + split_hit[-1]
                if genome_contig_id not in hits_by_contig:
                    hits_by_contig[genome_contig_id] = []
                hit_entry = {
                    'query_protein': query_protein,
                    'query_type': metadata['query_type'],
                    'is_nrps': metadata['is_nrps'],
                    'is_pks': metadata['is_pks'],
                    'start_position': int(start_position),
                    'end_position': int(end_position),
                    'locus': locus,
                    'genome_contig_id': genome_contig_id,
                    'window': float('inf')
                }
                hits_by_contig[genome_contig_id].append(hit_entry)
    if verbose:
        logging.debug(f"Grouped {len(all_required_proteins)} {query_type} proteins into {len(hits_by_contig)} contigs.")
    #if query_type == 'req' and not hits_by_contig:
        #raise NoRequiredProteinsGroupedContigsError("The required proteins were not grouped into contigs")
    
    return hits_by_contig, all_required_proteins

def make_lists_req_opt(
    req_hits_by_contig: Dict[str, List[Dict]],
    opt_hits_by_contig: Dict[str, List[Dict]],
    verbose: bool = False
) -> Tuple[List[str], List[Dict]]:
    """
    Combines hits from required and optional proteins grouped by contig, extracts their loci,
    and returns both the list of hits and their loci.

    Args:
        req_hits_by_contig (Dict[str, List[Dict]]): Dictionary containing lists of hit dictionaries
        for required proteins grouped by contig.
        opt_hits_by_contig (Dict[str, List[Dict]]): Dictionary containing lists of hit dictionaries
        for optional proteins grouped by contig.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Returns:
        Tuple[List[str], List[Dict]]:
            - List of loci extracted from all hits.
            - Combined list of all hits from required and optional proteins.
    """
    req_opt_hits = [
        item for sublist in req_hits_by_contig.values() for item in sublist
    ] + [
        item for sublist in opt_hits_by_contig.values() for item in sublist
    ]
    
    req_opt_locus = [hit['locus'] for hit in req_opt_hits]
    
    if verbose:
        logging.debug(f"Extracted {len(req_opt_locus)} loci from combined required and optional hits.")
            
    return req_opt_locus, req_opt_hits

def checking_all_required_proteins(
    init_window: List[Dict],
    all_required_proteins: Set[str],
    final_window: List[List[Dict]],
    verbose: bool = False
) -> None:
    """
    Checks if all proteins specified in all_required_proteins are present in the init_window.
    If they are, copies the init_window to the final_window. This ensures that only complete windows,
    containing all required proteins, are retained for further analysis.

    Args:
        init_window (List[Dict]): A list of dictionaries, each representing a hit, typically containing
                                  details such as the protein identifier, locus, and other metadata.
        all_required_proteins (Set[str]): A set containing identifiers of proteins that must be present
                                          in a window for it to be considered valid.
        final_window (List[List[Dict]]): A list where each element is a list of hits (dictionaries) that
                                         form a valid window containing all required proteins.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Returns:
        None: The function does not return anything but modifies final_window by appending valid windows.
    """
    gene_to_protein = {}
    for data in init_window:
        gene = data['locus']
        protein = data['query_protein']
        if gene not in gene_to_protein:
            gene_to_protein[gene] = set()
        gene_to_protein[gene].add(protein)
    present_proteins = {data['query_protein'] for data in init_window}    
    if all_required_proteins.issubset(present_proteins) and len(gene_to_protein) >= len(all_required_proteins):
        final_window.append(init_window.copy())
        if verbose:
            logging.debug("Added a new valid window containing all required proteins.")
            logging.debug(f"Window details: {init_window}")
    else:
        if verbose:
            missing_proteins = all_required_proteins - present_proteins
            logging.debug(f"Window missing required proteins: {missing_proteins}")
            logging.debug(f"Current window details: {init_window}")

def checking_distance_bw_loci(
    req_hits_by_contig: Dict[str, List[Dict]],
    all_required_proteins: Set[str],
    required_distance: float,
    verbose: bool = False
) -> List[List[Dict]]:
    """
    Evaluates hit dictionaries grouped by contig and defines windows based on a specified intergenic distance threshold
    and the presence of all required proteins. Windows are only considered valid if they contain all proteins from
    the all_required_proteins set and the distance between consecutive hits does not exceed the specified threshold.

    Args:
        req_hits_by_contig (Dict[str, List[Dict]]): Dictionary mapping contig identifiers to lists of hit dictionaries,
                                                    where each dictionary contains data about a protein hit.
        all_required_proteins (Set[str]): Set of protein identifiers that must be present in each considered window.
        required_distance (float): The maximum allowed distance between consecutive hits in a window; if exceeded, a new
                                   window is started.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Returns:
        List[List[Dict]]: A list of valid windows, where each window is a list of hit dictionaries that met all criteria.
                          If no windows meet the criteria, the program logs an informational message and raises an exception.

    Raises:
        NoValidWindowsError: If no valid windows are found, the function logs an informational message and raises this exception.
    """
    final_window = []
    for contig_id, hits in req_hits_by_contig.items():
        hits.sort(key=itemgetter('start_position'))
        init_window = []
        
        for idx, hit in enumerate(hits):
            if idx == 0 or (hit['start_position'] - hits[idx-1]['end_position']) > (required_distance * REQUIRED_DISTANCE_FACTOR):
                if init_window:
                    checking_all_required_proteins(init_window, all_required_proteins, final_window, verbose)
                init_window = [hit]
            else:
                init_window.append(hit)
        
        if init_window:
            checking_all_required_proteins(init_window, all_required_proteins, final_window, verbose)
            
    if not final_window:
        logging.info("No valid windows found. Try with different --query-cover/--identity values, or other combination of proteins, or play with the required_distance")
        raise NoValidWindowsError("No valid windows found.")

    if verbose:
        logging.info(f"Number of Windows Found: {len(final_window)}")
    
    return final_window

def process_windows(
        faa_dir: str,
        final_windows: List[Dict],
        genomes_list: List[str],
        output_directory: str,
        window_extension_factor: float,
        dbfaa: str,
        req_hits: List[str],
        opt_hits: List[str],
        verbose: bool = False
) -> Tuple[Dict, Dict, Dict, Dict, Dict, Dict]:
    """
    Processes genomic windows, writes GenBank files for each window, and tracks metadata for visualization and analysis.

    Arguments:
    faa_dir (str): Directory containing FAA files.
    final_windows (List[Dict]): List of dictionaries, each representing a window of genomic data.
    genomes_list (List[str]): List of genome file paths used to fetch sequence data.
    output_directory (str): Directory where output GenBank files will be saved.
    window_extension_factor (float): Factor by which the window's start and end positions are extended.
    dbfaa (str): Path to the FAA database for additional processing.
    req_hits (List[str]): List of required hits for feature processing.
    opt_hits (List[str]): List of optional hits for feature processing.

    Returns:
    Tuple of dictionaries containing various metadata about the processed windows.
    """
    if verbose:
        logging.info("Processing gator windows, writing GenBank files for each window, FASTA database, and tracks metadata for visualization and analysis")
    base_names = {os.path.basename(path): path for path in genomes_list}
    map_wc_wi, map_pa, map_ge, map_dmnd, map_lo_wn, map_win_size = {}, {}, {}, {}, {}, {}
    window_count = 0
    total_windows = len(final_windows)
    
    for windows in final_windows:
        window_count += 1
        genome_id, contig = windows[0]['genome_contig_id'].split('|-|')
        window_name = f"window_{window_count}--{os.path.splitext(genome_id)[0]}.gbff"
        start_positions, end_positions = extract_positions(windows)
        min_start, max_end = adjust_positions(start_positions, end_positions, window_extension_factor)
        try:
            window_record = process_windows_records(base_names[genome_id], contig, min_start, max_end, verbose)
        except KeyError:
            logging.error(f"Genome file {genome_id} not found in provided directories.")
            raise GenomeProcessingError(f"Genome file {genome_id} not found in provided directories.")


        loci_list, meta_loci = process_feature(window_record, genome_id, window_count, dbfaa, window_name, 
                                               req_hits, opt_hits, windows[0]['genome_contig_id'], 
                                               map_dmnd, map_lo_wn, verbose)
        write_genbank_files(output_directory, window_record, genome_id, window_count)
        map_pa[window_name] = loci_list
        map_wc_wi[genome_id] = window_name
        map_win_size[window_name] = len(window_record)
        map_ge[window_name] = {
            'window': f"window_{window_count}",
            'track_name': window_name[:-5],
            'record_length': len(window_record),
            'meta_loci': meta_loci
        }
        if verbose:
            logging.debug(f"Processed window {window_count}/{total_windows} for {window_name}.")

    return map_pa, map_wc_wi, map_ge, map_dmnd, map_lo_wn, map_win_size

def extract_positions(
        windows: List[Dict[str, int]]
) -> Tuple[List[int], List[int]]:
    """
    Extracts the start and end positions from a list of genomic windows.

    Arguments:
    windows (List[Dict[str, int]]): A list of dictionaries, each containing 'start_position' and 'end_position' keys.

    Returns:
    Tuple[List[int], List[int]]: Two lists containing the start and end positions of the genomic windows.
    """
    start_positions = [window['start_position'] for window in windows]
    end_positions = [window['end_position'] for window in windows]
    return start_positions, end_positions

def adjust_positions(
        start_positions: List[int],
        end_positions: List[int],
        window_extension_factor: float
) -> Tuple[int, int]:
    """
    Adjusts the start and end positions of genomic windows based on a given extension factor and base extension value.

    Arguments:
    start_positions (List[int]): List of start positions from genomic windows.
    end_positions (List[int]): List of end positions from genomic windows.
    window_extension_factor (float): Factor by which the window size is extended.
    
    Returns:
    Tuple[int, int]: Adjusted minimum start position and maximum end position of the genomic windows.
    """
    min_start = min(start_positions) - (args.we * window_extension_factor) 
    max_end = max(end_positions) + (args.we * window_extension_factor)
    return max(min_start, 0), max_end

def process_windows_records(
        genome: str,
        contig: str,
        min_start: int,
        max_end: int,
        verbose: bool = False
) -> Optional[SeqRecord]:
    """
    Extracts a genomic record segment based on provided start and end positions, handling contig edge cases.

    Arguments:
    genome (str): Path to the genome file in GenBank format.
    contig (str): Identifier for the contig within the genome file.
    min_start (int): Starting position of the window to extract.
    max_end (int): Ending position of the window to extract, adjusted not to exceed the contig length.

    Returns:
    A BioPython SeqRecord object sliced according to min_start and max_end, if found.

    Raises:
    FileNotFoundError: If the genome file does not exist.
    KeyError: If the specified contig is not found within the genome file.
    """
    try:
        with open(genome, 'r') as gf:
            found = False
            for rec in SeqIO.parse(gf, 'genbank'):
                if rec.id == contig:
                    found = True
                    max_end = min(max_end, len(rec))
                    for feat in rec.features:
                        if feat.type == "CDS":
                            feat.qualifiers["contig_edge"] = (feat.location.start <= EDGE_THRESHOLD or feat.location.end >= (len(rec) - EDGE_THRESHOLD))
                    sliced_record = rec[min_start:max_end]
                    if verbose:
                        logging.debug(f"Sliced record from {min_start} to {max_end} for contig {contig} in genome {genome}.")
                    return sliced_record
            if not found:
                logging.error(f"Window contig {contig} not found in {genome}. Ensure the correct database is used.")
                raise KeyError(f"Contig {contig} not found in {genome}")
    except FileNotFoundError:
        logging.error(f"Genome file {genome} not found. Ensure the genome file exists in the specified directory.")
        raise

def process_feature(
        window_record: SeqRecord,
        genome_id: str,
        window_count: int,
        dbfaa: str,
        window_name: str,
        req_hits: List[str],
        opt_hits: List[str],
        genome_contig: str,
        map_dmnd: Dict[str, Any],
        map_lo_wn: Dict[str, int],
        verbose: bool = False
) -> Tuple[List[str], List[Dict]]:
    """
    Processes features within a genomic window record, updating and mapping features based on specified hits.

    Args:
        window_record (SeqRecord): The BioPython SeqRecord object containing genomic features.
        genome_id (str): Identifier for the genome.
        window_count (int): Numeric identifier for the current window being processed.
        dbfaa (str): Path to the directory where FAA files will be saved.
        window_name (str): Name assigned to the current window, used for output files.
        req_hits (List[str]): List of required hit identifiers to process.
        opt_hits (List[str]): List of optional hit identifiers to process.
        genome_contig (str): Contig identifier associated with the current window.
        map_dmnd (Dict[str, Any]): Mapping of genomic features to Diamond DB results.
        map_lo_wn (Dict[str, int]): Mapping of loci to window numbers.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Returns:
        Tuple[List[str], List[Dict]]: 
            - loci_list: A list of loci identifiers processed.
            - meta_loci: A list of dictionaries containing metadata about each processed feature.

    Raises:
        Exception: General exceptions could be raised due to file operations or data inconsistencies.
    """
    loci_list, meta_loci = [], []
    try:
        for feature in window_record.features:
            if feature.type == "CDS":
                update_features_for_cds(feature, genome_id, window_count)
                locus_genome = process_hits_based_on_keys(feature, req_hits, genome_contig)
                locus_genome = process_hits_based_on_keys(feature, opt_hits, genome_contig)
                loci_list.append(locus_genome)
                sequence = calculate_translation_for_feature(feature, window_record, verbose)
                meta_cds = get_metadata_cds(feature, locus_genome)
                meta_loci.append(meta_cds)
                map_cds_features_to_dmnd(locus_genome, meta_cds, window_name, map_dmnd, window_record)
                write_fasta_dbfaa(dbfaa, locus_genome, sequence, window_name, verbose)
                map_lo_wn[locus_genome] = window_count
                if verbose:
                    logging.debug(f"Processed feature {locus_genome} in genome {genome_id} for window {window_name}")
    except Exception as e:
        logging.error(f"Error processing features in genome {genome_id} for window {window_name}: {e}")
        raise
    
    return loci_list, meta_loci

def update_features_for_cds(
        feature: SeqFeature,
        genome_id: str,
        window_count: int
) -> None:
    """
    Initialize feature qualifiers for genomic features and append a unique identifier.
    
    Arguments:
    feature (SeqFeature): The feature to be updated.
    genome_id (str): The genome identifier to append to locus and protein tags.
    window_count (int): The count of the window, appended as metadata.
    """
    init_qualifiers(feature, ["gator_query", "gator_nrps", "gator_pks", "gator_hit", "gator_window"], 
                    default_values=[None, False, False, None, window_count])
    append_suffix_to_locus(feature, ['locus_tag', 'protein_id'], f"|_|{genome_id}")
    
def init_qualifiers(
        feature: SeqFeature,
        qualifiers: List[str],
        default_values: List
) -> None:
    """
    Initialize multiple qualifiers with specified default values for a genomic feature.
    
    Arguments:
    feature (SeqFeature): The feature whose qualifiers are to be initialized.
    qualifiers (List[str]): A list of qualifier names to initialize.
    default_values (List): A list of default values corresponding to the qualifiers.
    """
    for qualifier, default_value in zip(qualifiers, default_values):
        feature.qualifiers[qualifier] = default_value
    
def append_suffix_to_locus(
        feature: SeqFeature,
        qualifiers: List[str],
        suffix: str
) -> None:
    """
    Appends a genome name to specified locus to made it unique.
    
    Arguments:
    feature (SeqFeature): The feature whose qualifiers may be modified.
    qualifiers (List[str]): A list of qualifier names to append the suffix to.
    suffix (str): The suffix to append to qualifier values.
    """
    for qualifier in qualifiers:
        if qualifier in feature.qualifiers:
            feature.qualifiers[qualifier][0] += suffix

def process_hits_based_on_keys(
        feature: SeqFeature,
        hits: Dict[str, Any],
        genome_contig: str
) -> str:
    """
    Process hits against feature qualifiers based on the genomic contig. Makes gator feature qualifiers annotations.

    Arguments:
    feature (SeqFeature): The feature to update based on hit data.
    hits (Dict[str, Any]): Dictionary of hits mapped by contig.
    genome_contig (str): The contig identifier to filter hits.

    Returns:
    str: The locus genome identifier for the feature.
    """
    locus_tag = feature.qualifiers.get('locus_tag', [''])[0]
    locus_genome = locus_tag if locus_tag else feature.qualifiers.get('protein_id', [''])[0]
    for hit in hits.get(genome_contig, []):
        if hit['locus'] == locus_genome:
            update_qualifiers_based_on_hits(feature, hit)
    return locus_genome
            
def update_qualifiers_based_on_hits(
        feature : SeqFeature,
        hit: dict
) -> None:
    """
    Updates specific qualifiers of a genomic feature based on conditions matched from a given hit.

    Arguments:
        feature (SeqFeature): The genomic feature whose qualifiers are to be updated.
        hit (dict): A dictionary containing hit data that may influence the feature's qualifiers.

    This function modifies the 'gator_query', 'gator_nrps', 'gator_pks', and 'gator_hit' qualifiers of the feature
    based on the provided hit data. It logs any changes made for tracking and debugging purposes.
    """
    updates = {}
    if "gator_query" in feature.qualifiers and "query_type" in hit:
        current_query = feature.qualifiers["gator_query"]
        new_query = hit["query_type"]
        if new_query != current_query:
            updates["gator_query"] = new_query

    if "gator_nrps" in feature.qualifiers and "is_nrps" in hit:
        current_nrps = feature.qualifiers["gator_nrps"]
        new_nrps = hit["is_nrps"]
        if new_nrps != current_nrps:
            updates["gator_nrps"] = new_nrps

    if "gator_pks" in feature.qualifiers and "is_pks" in hit:
        current_pks = feature.qualifiers["gator_pks"]
        new_pks = hit["is_pks"]
        if new_pks != current_pks:
            updates["gator_pks"] = new_pks

    if "gator_hit" in feature.qualifiers and "query_protein" in hit:
        current_hit = feature.qualifiers["gator_hit"]
        new_hit = hit["query_protein"] if not hit["is_nrps"] and not hit["is_pks"] else None
        if new_hit != current_hit:
            updates["gator_hit"] = new_hit
    feature.qualifiers.update(updates)

def add_trailing_N(
        sequence: str,
        verbose: bool = False
) -> str:
    """Add trailing 'N' characters to make the sequence length a multiple of three.
        
    Args:        
        sequence (str): The DNA sequence to be padded.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Returns:
        str: The padded DNA sequence.
    """
    if len(sequence) % 3 != 0:
        padding_length = 3 - len(sequence) % 3
        sequence += 'N' * padding_length
        if verbose:
            logging.debug(f"Added {padding_length} 'N' characters to sequence to make its length a multiple of three.")
    return sequence
    
def calculate_translation_for_feature(
        feature: SeqFeature,
        window_record: SeqRecord,
        verbose: bool = False
) -> str:
    """
    Calculates the amino acid translation for a given genomic feature based on its DNA sequence.

    Args:
        feature (SeqFeature): The feature for which to calculate the translation.
        window_record (SeqRecord): The record from which the feature's sequence will be extracted.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Returns:
        str: The translated amino acid sequence. If an error occurs during the translation process, logs a warning and returns None.
    """
    seq = None
    try:
        if isinstance(feature.location, CompoundLocation) or 'translation' in feature.qualifiers:
            if verbose:
                logging.debug(f"Translation found in qualifiers for feature at {feature.location}. Using existing translation.")
            return feature.qualifiers['translation'][0]
        
        if feature.location.strand == 1:
            seq = str(feature.extract(window_record).seq)
        else:
            seq = str(feature.extract(window_record).seq.reverse_complement())

        seq = add_trailing_N(seq, verbose)
        seq = str(Seq(seq).translate(to_stop=True))
        
        feature.qualifiers["translation"] = [seq]    
        if verbose:
            logging.debug(f"Calculated translation for feature at {feature.location}.")
        
        return seq

    except Exception as e:
        logging.debug(f"Skipping {feature.location} due to compound location: {e}")
        
def get_metadata_cds(
        feature: SeqFeature,
        locus: str
) -> Tuple[int, int, int, str, str, List[str], List[str], List[str], str, List[str]]:
    """
    Extracts metadata for a CDS (coding sequence) feature.

    Arguments:
        feature (SeqFeature): The feature from which metadata is extracted.
        locus (str): The locus identifier associated with the feature.

    Returns:
        tuple: A tuple containing:
            - Start position (int)
            - End position (int)
            - Strand (int)
            - Locus (str)
            - Product name (str)
            - Gator query (list)
            - Gator NRPS (list)
            - Gator PKS (list)
            - Gator hit (str)
            - Contig edge flag (list)
    
    Ensures that if any qualifiers are missing, default values are provided to maintain consistency.
    """
    try:
        start = int(feature.location.start)
        end = int(feature.location.end)
        strand = feature.location.strand
        product = feature.qualifiers['product'] if 'product' in feature.qualifiers else 'no_annotation'
        gator_query = feature.qualifiers['gator_query'] if 'gator_query' in feature.qualifiers else []
        gator_nrps = feature.qualifiers['gator_nrps'] if 'gator_nrps' in feature.qualifiers else []
        gator_pks = feature.qualifiers['gator_pks'] if 'gator_pks' in feature.qualifiers else []
        gator_hit = feature.qualifiers['gator_hit'] if 'gator_hit' in feature.qualifiers else 'None'
        contig_edge = feature.qualifiers['contig_edge'] if 'contig_edge' in feature.qualifiers else []

        return (start, end, strand, locus, product, gator_query, gator_nrps, gator_pks, gator_hit, contig_edge)

    except KeyError as e:
        logging.error(f"Missing expected qualifier in feature: {e}")
        raise

def write_fasta_dbfaa(
        fh: TextIO,
        header: str,
        sequence: str,
        window_name: str,
        verbose: bool = False
) -> None:
    """
    Writes a FASTA format entry to an open file handle.

    Arguments:
        fh (TextIO): An open writable file handle where the FASTA entry will be written.
        header (str): The header for the FASTA entry, typically containing the sequence identifier.
        sequence (str): The amino acid sequence to write.
        window_name (str): Additional descriptor appended to the header to uniquely identify the window.
        verbose (bool): If True, enables verbose logging.

    This function assumes that the file handle provided is open and writable. It performs checks
    to ensure the header and sequence are non-empty before writing.
    """
    if not header:
        if verbose:
            logging.debug("Attempted to write FASTA entry with empty header.")
        return
    
    if not sequence:
        if verbose:
            logging.debug("Attempted to write FASTA entry with empty sequence.")
        return
    
    try:
        fh.write(f'>{header}|-|{window_name}\n{sequence}\n')
    except Exception as e:
        logging.error(f"Failed to write FASTA entry for {header}: {e}")

def map_cds_features_to_dmnd(
        locus_genome: str,
        meta_cds: Tuple[int, int, int, str, str, str, bool, bool],
        window_name: str,
        map_dm: Dict[str, Dict[str, Any]],
        window_record: SeqRecord
) -> None:
    """
    Maps coding sequence feature metadata to a specified dictionary structured by window names.

    Args:
        locus_genome (str): Identifier for the locus associated with the CDS feature.
        meta_cds (tuple): Metadata about the CDS feature including start, end, strand, and biochemical properties.
        window_name (str): The name of the window to which the CDS feature belongs.
        map_dm (dict): The dictionary to which CDS feature metadata is being mapped.
        window_record: The genomic record associated with the window, used to determine the record length.
        verbose (bool): Flag to enable logging of detailed execution messages.

    This function updates the map_dm dictionary with metadata about the CDS feature, structuring data by window names
    and locus identifiers.
    """
    if window_name not in map_dm:
        map_dm[window_name] = {}
    map_dm[window_name][locus_genome] = {
        'nrps': meta_cds[6], 
        'pks': meta_cds[7],  
        'start': meta_cds[0],
        'end': meta_cds[1],  
        'strand': meta_cds[2],
        'record_length': len(window_record) 
    }

def write_genbank_files(
        directory: str,
        window_record: SeqRecord,
        basename: str,
        count: int
) -> None:
    """
    Writes a genomic window record to a GenBank file in the specified directory.

    Arguments:
        directory (str): The directory where the GenBank file should be saved.
        window_record (SeqRecord): The BioPython SeqRecord object to write.
        basename (str): The base name for the file, which will be appended with window count.
        count (int): The window count, used to differentiate filenames.

    Ensures the directory exists (creates if necessary), then writes the record to a GenBank file.
    """
    try:
        os.makedirs(directory, exist_ok=True)
        file_path = os.path.join(directory, f"window_{count}--{basename}")
        with open(file_path, 'w') as fh:
            SeqIO.write(window_record, fh, 'genbank')
    except Exception as e:
        logging.error(f"Failed to write GenBank file at {file_path}: {e}")
        raise RuntimeError(f"Failed to write GenBank file due to an error: {e}")

def get_min_window(
        dictionaries: List[Dict[str, Any]]
) -> int:
    """
    Determines the minimum window value from a list of dictionaries containing hit information.

    Args:
        dictionaries (List[Dict[str, Any]]): List of dictionaries, each representing a hit with a 'window' key.

    Returns:
        int: The minimum window number found in the dictionaries.
    """
    return min(hits['window'] for hits in dictionaries)


def prepare_hits_to_write(
        all_hits: List[Dict[str, Any]],
        map_lo_wn: Dict[str, int],
        windows_tsv: str,
        verbose: bool = False
) -> None:
    """
    Organizes hits by genomic contig and window, then writes them to a TSV file.

    Args:
        all_hits (List[Dict[str, Any]]): List containing hit dictionaries.
        map_lo_wn (Dict[str, int]): Mapping from locus to window number.
        windows_tsv (str): File path where the TSV will be written.
    """
    grouped = {}
    for hits in all_hits:
        if hits['locus'] in map_lo_wn:
            hits['window'] = map_lo_wn[hits['locus']]
        if hits['genome_contig_id'] not in grouped:
            grouped[hits['genome_contig_id']] = []
        grouped[hits['genome_contig_id']].append(hits)
        grouped[hits['genome_contig_id']].sort(key=itemgetter('start_position'))
    
    sorted_by_window = sorted(grouped.values(), key=get_min_window)
    write_table_hits(all_hits, sorted_by_window, windows_tsv, verbose)

def write_table_hits(
        all_hits: List[Dict[str, Any]],
        sorted_by_window: List[List[Dict[str, Any]]],
        windows_tsv: str,
        verbose: bool = False
) -> None:
    """
    Writes the organized hits to a TSV file.

    Args:
        all_hits (List[Dict[str, Any]]): List containing all hit dictionaries.
        sorted_by_window (List[List[Dict[str, Any]]]): List of lists of hit dictionaries sorted by window.
        windows_tsv (str): File path where the TSV will be written.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Raises:
        IOError: If an error occurs during file writing.
    """
    try:
        flat_list = [d for sublist in sorted_by_window for d in sublist]
        for dictionary in flat_list:
            dictionary['window'] = 'NA' if dictionary['window'] == float('inf') else dictionary['window']    
        with open(windows_tsv, 'w', newline='') as f_output:
            window_opt_hits_tsv = csv.DictWriter(f_output, fieldnames=all_hits[0].keys(), delimiter='\t')
            window_opt_hits_tsv.writeheader()
            window_opt_hits_tsv.writerows(flat_list)
        
        if verbose:
            logging.info(f"Successfully wrote gator table with metadata to {windows_tsv}")
    
    except IOError as e:
        logging.error(f"Error writing to {windows_tsv}: {e}")
        raise

def create_diamond_database(
        db_faa: str,
        database_name: str,
        need_return: bool,
        threads: int,
        verbose: bool = False
) -> Optional[str]:
    """
    Creates a DIAMOND database from a provided FASTA file.

    Args:
        db_faa (str): Path to the input FASTA file containing protein sequences.
        database_name (str): Name of the DIAMOND database to be created.
        need_return (bool): If True, the function returns the name of the database created.
        threads (int): Number of threads to use for DIAMOND database creation.
        verbose (bool): Flag to enable logging of detailed execution messages.

    Returns:
        Optional[str]: The name of the database if need_return is True, otherwise None.

    Raises:
        RuntimeError: If the DIAMOND command fails.
    """
    try:
        command = [
            'diamond', 'makedb',
            '--in', db_faa,
            '--db', database_name,
            '--quiet',
            '--threads', str(threads)
        ]
        subprocess.run(command, check=True)
        
        if verbose:
            logging.debug(f"Successfully created DIAMOND database: {database_name}")

        if need_return:
            return database_name

    except subprocess.CalledProcessError as e:
        logging.error(f"Failed to create DIAMOND database from {db_faa}: {e}")
        raise RuntimeError(f"DIAMOND database creation failed: {e}")

def custom_sort_key(
        window: str
) -> Tuple[float, str]:
    """
    Defines a sorting key for genomic windows based on size and identifier.

    Args:
        window (str): The window identifier, expected to contain information split by '--'
                      and ending in a specific format (e.g., '.gbk' which is stripped).

    Returns:
        Tuple[float, str]: A tuple containing the size criterion from map_win_size and
                           a substring from the window identifier.
    """
    size_criterion = map_win_size[window] if window in map_win_size else float('inf')
    full_string_after_dash = window.split('--', 1)[1] if '--' in window else window
    return size_criterion, full_string_after_dash

def deduplication_for_windows(
        directory_output: str,
        dbfaa: str,
        dmnddb: str,
        raw_map_pa: Dict[str, List[str]],
        req_opt_locus: Dict[str, Any],
        output: str,
        newdbfaa: str,
        map_dmnd: Dict[str, Any],
        map_win_size: Dict[str, int],
        verbose: bool = False
) -> Set[str]:
    """
    Performs deduplication process for genomic windows to minimize redundancy in database entries.

    Arguments:
        directory_output (str): Directory for temporary and final outputs.
        dbfaa (str): Path to the initial FAA database.
        dmnddb (str): Path to the initial Diamond database.
        raw_map_pa (Dict[str, List[str]]): mapping of window_name with loci.                                                                                
        req_opt_locus: Required and optional locus information.
        output: General output path or identifier.
        newdbfaa (str): Path to the new, deduplicated FAA database.
        map_dmnd: Mapping for Diamond results.
        map_win_size: Mapping of window sizes to window identifiers.

    Returns:
        A set of unique windows after deduplication.
    """
    if verbose:
        logging.info("Starting deduplication process for gator windows.")
    current_dmnddb = dmnddb
    current_dbfaa = dbfaa
    no_repre = ''
    unq_gfs, itself_comp = {}, {}
    full_set = set()
    sorted_windows = sorted(list(map_win_size.keys()), key=custom_sort_key, reverse=True)
    sorted_windows = [item[:-5] for item in sorted_windows]
    biggest_window = sorted_windows.pop(0)
    n = 0
    next_faa = None
    with TemporaryDirectory(dir=directory_output) as temp_dir:
        initial_repre = get_faa(temp_dir, dbfaa, no_repre, biggest_window, verbose)
        repre_list = [initial_repre]
        for faa in repre_list:
            make_newdbfaa(faa, newdbfaa, verbose)
            faa_name = os.path.basename(faa).split('.')[0]
            n += 1
            diamond_output = os.path.join(temp_dir, f"{faa_name}.dmnd")
            run_diamond(faa, current_dmnddb, diamond_output, args.t, args.qc, args.idt, False, args.bs, args.k, verbose)
            os.remove(faa)
            pa, idx_win_map = parse_diamond_pa(diamond_output, {}, raw_map_pa, output, False, False, map_dmnd, verbose)
            os.remove(diamond_output)
            gfs, unq_gfs, itself_comp = get_gfs(pa, req_opt_locus, directory_output, idx_win_map, unq_gfs, itself_comp, True, False, verbose)
            full_set = write_concat_gfs(directory_output, gfs, idx_win_map, full_set, n, True)
            if full_set.issubset(set(unq_gfs.keys())):
                if verbose:
                    logging.info("All windows have been processed and deduplicated.")
                break
            current_dbfaa, current_dmnddb = reduce_databases(current_dbfaa, unq_gfs, temp_dir, n, faa_name, sorted_windows, args.t, verbose)
            if sorted_windows:
                biggest_window = sorted_windows.pop(0)
                next_faa = get_faa(temp_dir, current_dbfaa, faa_name, biggest_window)
            if next_faa and next_faa not in repre_list:
                repre_list.append(next_faa)
        clean_up(current_dbfaa, current_dmnddb)
    unq_windows = get_unique_comp(os.path.join(directory_output, 'deduplication_data', 'unq_comp.tsv'), unq_gfs, itself_comp, verbose)
    return unq_windows

def reduce_databases(
        dbfaa: str,
        unq_gfs: Dict[str, Any],
        temp_dir: str,
        iteration: int,
        prefix: str,
        sorted_windows: List[str],
        threads: int,
        verbose: bool = False
) -> Tuple[str, str]:
    """
    Reduces FAA and DMND databases by creating subsets for further processing.

    Args:
        dbfaa (str): Path to the original FAA database.
        unq_gfs (Dict[str, Any]): Dictionary of unique genomic features.
        temp_dir (str): Temporary directory to store subset databases.
        iteration (int): Current iteration number to label subset databases.
        prefix (str): Prefix to use for naming the subset files.
        sorted_windows (List[str]): List of sorted window identifiers for processing.
        threads (int): Number of threads to use for DIAMOND database creation.
        verbose (bool): Flag to enable verbose logging.

    Returns:
        Tuple[str, str]: Paths to the new FAA and DMND database files.
    """
    new_dbfaa = subset_dbfaa(dbfaa, unq_gfs, os.path.join(temp_dir, f'subset_{iteration}.faa'), prefix, sorted_windows, verbose)
    new_dmnddb = create_diamond_database(new_dbfaa, os.path.join(temp_dir, f'subset_{iteration}.dmnd'), True, threads, verbose)
    return new_dbfaa, new_dmnddb

def clean_up(
        current_dbfaa: str,
        current_dmnddb: str
) -> None:
    """
    Removes specified database files as part of cleanup.

    Args:
        current_dbfaa (str): Path to the FAA database to remove.
        current_dmnddb (str): Path to the DMND database to remove.
    """
    try:
        os.remove(current_dbfaa)
        os.remove(current_dmnddb)
    except OSError as e:
        logging.error(f"Failed to remove databases {current_dbfaa} or {current_dmnddb}: {e}")

def get_unique_comp(
        directory_output: str,
        unq_dict: Dict[str, str],
        itself_comp : Dict[str, str],
        verbose: bool = False
) -> Set[str]:
    """
    Writes unique computations to a file and collects the deduplicate gator windows.

    Args:
        directory_output (str): Path to the output file for unique computations.
        unq_dict (Dict[str, str]): Dictionary containing the result of deduplication process, unique comparisions for gator focal scores.
        itself_comp (Dict[str, str]): Dictionary containing the keys that can overwritte unq_dict, because itfself gator focal scores comparisions (w1 vs w1).
        verbose (bool): Flag to enable verbose logging.

    Returns:
        Set[str]: Set of deduplicate gator windows.
    """
    unq_windows = set()
    try:
        with open(directory_output, 'w') as oh:
            for repre in unq_dict:
                oh.write(f"{repre}\t{unq_dict[repre]}\n")
                unq_windows.add(unq_dict[repre] + '.gbff')
            for itself in itself_comp:
                oh.write(f"{itself}\t{itself_comp[itself]}\n")
                unq_windows.add(itself_comp[itself] + '.gbff')
        if verbose:
            logging.info(f"Number of unique gator windows after deduplication: {len(unq_windows)}")
            logging.info(f"Unique computations written to {directory_output}")
    except IOError as e:
        logging.error(f"Failed to write to {directory_output}: {e}")

    return unq_windows

def get_faa(
        directory_output: str,
        dbf: str,
        repre_faa: str,
        biggest_window: str,
        verbose: bool = False
) -> Optional[str]:
    """
    Extracts a specific sequence from a FASTA file based on the biggest window identifier and writes it to a new file.

    Args:
        directory_output (str): Directory where the new FASTA file will be saved.
        dbf (str): Path to the original FASTA file.
        repre_faa (str): Identifier of a representative FAA, which should not be the same as the biggest window.
        biggest_window (str): The identifier of the window from which the sequence should be extracted.

    Returns:
        Optional[str]: Path to the new FAA file if successful, None otherwise.
    """
    repre = None
    lines_to_write = []
    sequence_started = False
    try:
        with open(dbf, 'r') as fh:
            for line in fh:
                if line.startswith('>'):
                    window = line.strip().split('|-|')[-1][:-5]
                    if repre is None and window == biggest_window and window != repre_faa:
                        repre = window
                        sequence_started = True
                    elif window != repre and repre is not None:
                        break 
                if sequence_started:
                    lines_to_write.append(line)
    except FileNotFoundError:
        logging.error(f"File not found: {dbf}")
        return None
    except Exception as e:
        logging.error(f"Error reading from file {dbf}: {e}")
        return None

    if repre:
        filepath = os.path.join(directory_output, f'{repre}.faa')
        try:
            with open(filepath, 'w') as out_fh:
                out_fh.writelines(lines_to_write)
                if verbose:
                    logging.debug(f"Successfully wrote FAA file: {filepath}")
            return filepath
        except Exception as e:
            logging.error(f"Failed to write FAA file {filepath}: {e}")
            return None
    else:
        if verbose:
            logging.warning("No matching window found or no sequence started.")
        return None
                                                                                       
def parse_diamond_pa(
        dmnd_out: str,
        pa: Dict[str, Any],
        raw_map_pa: Dict[str, List[str]],
        output: str,
        write: bool,
        figures: bool,
        map_dmnd: Dict[str, Any],
        verbose: bool = False
) -> Tuple[Dict[str, Any], Dict[int, str], Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
    """
    Parses DIAMOND output to update protein alignments data and optionally generate figures and write outputs.

    Args:
        dmnd_out (str): Path to the DIAMOND output file.
        pa (Dict[str, Any]): Dictionary to be updated with parsed data.
        raw_map_pa (Dict[str, List[str]]): mapping of window_name with loci.
        output (str): Output directory or file path for writing results.
        write (bool): Whether to write output tables.
        figures (bool): Whether to generate figures.
        map_dmnd (Dict[str, Any]): Mapping dictionary for DIAMOND data.
        verbose (bool): If True, enables verbose logging.

    Returns:
        Tuple[Dict[str, Any], Dict[int, str], Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
            - Updated 'pa' dictionary.
            - Index to window name mapping dictionary.
            - (Optional) Windows identity dictionary if 'figures' is True.
            - (Optional) All hits dictionary if 'figures' is True.
    """    
    win_list = list(raw_map_pa.keys())
    win_index_map = {win: idx for idx, win in enumerate(win_list)}
    idx_win_map = {idx: win for idx, win in enumerate(win_list)}
    init_needed = set(raw_map_pa.keys()) - set(pa.keys())
    windows_identity, all_hits = {}, {}
    try:
        with open(dmnd_out, 'r') as fh:
            for line in fh:
                cols = line.strip().split()
                process_diamond_line(cols, pa, init_needed, raw_map_pa, win_list, win_index_map, windows_identity, all_hits, map_dmnd, figures, verbose)
        if write:
            if verbose:
                logging.debug(f"Writing PA tables to {output}")
            write_pa_tables(pa, output)
        if figures:
            return pa, idx_win_map, windows_identity, all_hits
        else:
            return pa, idx_win_map
    except FileNotFoundError:
        logging.error(f"File not found: {dmnd_out}")
        raise
    except Exception as e:
        logging.error(f"Error processing file {dmnd_out}: {e}")
        raise

def process_diamond_line(
        cols: List[str],
        pa: Dict[str, Any],
        init_needed: set,
        raw_map_pa: Dict[str, Any],
        win_list: List[str],
        win_index_map: Dict[str, int],
        windows_identity: Dict[str, Any],
        all_hits: Dict[str, Dict[str, float]],
        map_dmnd: Dict[str, Dict[str, Any]],
        figures: bool,
        verbose: bool = False
) -> None:
    """
    Processes a line from DIAMOND output, updating various mappings and data structures.

    Args:
        cols (List[str]): Columns from a line in the DIAMOND output.
        pa (Dict[str, Any]): Dictionary to be updated with parsed data to create a presence absence data structure.
        init_needed (set): Set of keys that need to be initialized in 'pa'.
        raw_map_pa (Dict[str, List[str]]): mapping of window_name with loci.                                                                  
        win_list (List[str]): List of window identifiers.
        win_index_map (Dict[str, int]): Mapping from window identifiers to indices.
        windows_identity (Dict[str, Any]): Dictionary to store windows identity data.
        all_hits (Dict[str, Dict[str, float]]): Dictionary to store all hit data.
        map_dmnd (Dict[str, Dict[str, Any]]): Mapping dictionary for DIAMOND data.
        figures (bool): Whether to generate figures.
        verbose (bool): If True, enables verbose logging.

    Returns:
        None
    """
    qhit, hhit, per_identity, query_aa_start, query_aa_end, hit_aa_start, hit_aa_end, bit_score = (
        cols[0], cols[1], float(cols[2]), int(cols[6]), int(cols[7]), int(cols[8]), int(cols[9]), float(cols[11])
    )
    ql, hl = qhit.split('|-|')[0], hhit.split('|-|')[0]
    qw, hw = qhit.split('|-|')[-1], hhit.split('|-|')[-1]

    if qw in init_needed:
        initialize_pa_data_structure(qw, raw_map_pa, pa, win_list, win_index_map, init_needed)
    if qw in pa and ql in pa[qw]['loci'] and hw in pa[qw]['win_index_map']:
        win_index = pa[qw]['win_index_map'][hw]
        pa[qw]['loci'][ql][win_index] = 1
        if verbose:
            logging.debug(f"Updated presence hit data for {ql} in {qw}")
    if figures:
        if verbose:
            logging.debug(f"Updating windows identity for {qw} and {hw}")
        windows_identity = meta_homology_rails(
            windows_identity, qw, hw, map_dmnd[qw][ql]['nrps'], map_dmnd[qw][ql]['pks'],
            map_dmnd[qw][ql]['start'], map_dmnd[qw][ql]['end'], map_dmnd[qw][ql]['strand'],
            query_aa_start, query_aa_end, map_dmnd[hw][hl]['nrps'], map_dmnd[hw][hl]['pks'],
            map_dmnd[hw][hl]['start'], map_dmnd[hw][hl]['end'], map_dmnd[hw][hl]['strand'],
            hit_aa_start, hit_aa_end, per_identity, map_dmnd[qw][ql]['record_length'], 
            map_dmnd[hw][hl]['record_length'], bit_score, ql, hl
        )
        all_hits = get_all_hits(all_hits, ql, hl, per_identity)
        if verbose:
            logging.debug(f"Updated all hits for {ql} and {hl}")

def initialize_pa_data_structure(
        qw: str,
        raw_map_pa: Dict[str, List[str]],
        pa: Dict[str, Dict[str, Any]],
        win_list: List[str],
        win_index_map: Dict[str, int],
        init_needed: Set[str]
) -> None:
    """
    Initializes data structures for storing presence absence data.

    Args:
        qw (str): Query window identifier.
        raw_map_pa (Dict[str, List[str]]): mapping of window_name with loci.
        pa (Dict[str, Dict[str, Any]]): Dictionary to be initialized with parsed data.
        win_list (List[str]): List of window identifiers.
        win_index_map (Dict[str, int]): Mapping from window identifiers to indices.
        init_needed (Set[str]): Set of keys that need to be initialized in 'pa'.

    Returns:
        None: The function modifies 'pa' and 'init_needed' in place.
    """
    loci_list = raw_map_pa[qw]
    pa[qw] = {
        'loci': {locus: np.zeros(len(win_list)) for locus in loci_list},
        'win_index_map': win_index_map
    }
    init_needed.remove(qw)

def write_pa_tables(
        pa: Dict[str, Dict[str, Any]],
        output: str,
        verbose: bool = False
) -> None:
    """
    Writes parsed alignment data to CSV files, one for each unique identifier in the data.

    Args:
        pa (Dict[str, Dict[str, Any]]):  Dictionary containing presence absence data with keys as window identifiers.
        output (str): The directory path where CSV files should be saved.
        verbose (bool): If True, additional logging information will be output.
    
    This function ensures that the output directory exists, creates a CSV file for each identifier,
    and writes data including a header and rows for each locus and its presence absence data.
    """
    os.makedirs(output, exist_ok=True)
    for qw in pa:
        filename = f"{qw[:-5]}.csv"
        output_file = os.path.join(output, filename)
        loci = pa[qw]['loci']
        win_index_map = pa[qw]['win_index_map']
        win_list = list(win_index_map.keys())
        try:
            with open(output_file, 'w', newline='') as oh:
                writer = csv.writer(oh)
                header = ['Gator Windows/Focal Loci'] + list(loci.keys())
                writer.writerow(header)
                for i, win in enumerate(win_list):
                    row = [win] + [loci[locus][i] for locus in loci]
                    #if any(value != 0 for value in row[1:]): 
                    writer.writerow(row)
            if verbose:
                logging.debug(f"Successfully wrote file {output_file}")
        except IOError as e:
            logging.error(f"Failed to write file {output_file}: {e}")
            raise 

def meta_homology_rails(
    windows_identity: Dict[str, Dict[str, List[Dict]]],
    query_window: str,
    hit_window: str,
    query_nrps: bool,
    query_pks: bool,
    query_start: int,
    query_end: int,
    query_strand: int,
    query_aa_start: int,
    query_aa_end: int,
    hit_nrps: bool,
    hit_pks: bool,
    hit_start: int,
    hit_end: int,
    hit_strand: int,
    hit_aa_start: int,
    hit_aa_end: int,
    per_identity: float,
    record_length_query: int,
    record_length_hit: int,
    bit_score: float,
    locus_query: str,
    locus_hit: str
) -> Dict[str, Dict[str, List[Dict]]]:
    """
    Updates the windows identity mapping with homology details between query and hit windows.

    Args:
        windows_identity (Dict): The main dictionary holding homology rails data.
        query_window (str): Identifier of the query window.
        hit_window (str): Identifier of the hit window.
        query_nrps (bool), query_pks (bool): NRPS and PKS flags for the query.
        query_start, query_end, query_strand, query_aa_start, query_aa_end (int): Query genomic and amino acid positions.
        hit_nrps (bool), hit_pks (bool): NRPS and PKS flags for the hit.
        hit_start, hit_end, hit_strand, hit_aa_start, hit_aa_end (int): Hit genomic and amino acid positions.
        per_identity (float): Percentage identity between query and hit.
        record_length_query, record_length_hit (int): Record lengths of the query and hit.
        bit_score (float): Bit score of the homology hit.
        locus_query, locus_hit (str): Loci identifiers for the query and hit.

    Returns:
        Dict: Updated windows identity dictionary.
    """
    if query_window not in windows_identity:
        windows_identity[query_window] = {}
    if hit_window not in windows_identity[query_window]:
        windows_identity[query_window][hit_window] = []

    windows_identity[query_window][hit_window].append({
        'query_window': query_window[:-5],
        'query_nrps': query_nrps,
        'query_pks': query_pks,
        'query_start': query_start,
        'query_end': query_end,
        'query_strand': query_strand,
        'query_aa_start': query_aa_start,
        'query_aa_end': query_aa_end,
        'hit_window': hit_window[:-5],
        'hit_nrps': hit_nrps,
        'hit_pks': hit_pks,
        'hit_start': hit_start,
        'hit_end': hit_end,
        'hit_strand': hit_strand,
        'hit_aa_start': hit_aa_start,
        'hit_aa_end': hit_aa_end,
        'per_identity': per_identity,
        'record_length_query': record_length_query,
        'record_length_hit': record_length_hit,
        'bit_score': bit_score,
        'locus_query': locus_query,
        'locus_hit': locus_hit
    })

    return windows_identity

def get_all_hits(
        all_hits: Dict[str, Dict[str, float]],
        locus_query: str,
        locus_hit: str,
        per_identity: float
) -> Dict[str, Dict[str, float]]:
    """
    Updates a dictionary with the percentage identity between query and hit loci.

    Args:
        all_hits (Dict[str, Dict[str, float]]): Dictionary to store all hit information with percent identities.
        locus_query (str): Identifier for the query locus.
        locus_hit (str): Identifier for the hit locus.
        per_identity (float): The percentage identity between the query and the hit locus.

    Returns:
        Dict[str, Dict[str, float]]: Updated dictionary with new hit information added.

    This function ensures that for each query locus, there is a sub-dictionary that holds the percent identity
    to each corresponding hit locus.
    """
    if locus_query not in all_hits:
        all_hits[locus_query] = {}
    all_hits[locus_query][locus_hit] = per_identity
    return all_hits

def get_gfs(
    gfs: Dict[str, Dict[str, Any]],
    req_opt: List[str],
    directory_output: str,
    idx_win_map: Dict[int, str],
    deduplicated_wins: Dict[str, str],
    itself_comp: Dict[str, str],
    deduplication: bool,
    write: bool,
    verbose: bool = False
) -> Tuple[Dict[str, Dict[str, Any]], Optional[Dict[str, str]], Optional[Dict[str, str]]]:
    """
    Processes genomic features to calculate and potentially write out genomic feature scores (GFS).

    Args:
        gfs: Dictionary containing the presence absence data.
        req_opt: List of required or optional features.
        directory_output: Path where outputs will be saved.
        idx_win_map: Dictionary mapping index to window identifiers.
        deduplicated_wins: Dictionary tracking unique windows with the identical.
        itself_comp: Dictionary tracking the values that are overwriting deduplicated_wins, because itself window comparision for focal and window (w1 vs w1).
        deduplication: Boolean indicating if deduplication process should be processed.
        write: Boolean indicating if results should be written to file.

    Returns:
        Tuple containing updated genomic features and deduplicate windows if deduplication is True.
    """
    gator_scores_path = os.path.join(directory_output, 'gator_scores')
    os.makedirs(gator_scores_path, exist_ok=True)
    for key, pa in gfs.items():
        processed_loci = set()
        weight_arrays = []
        for locus in req_opt:
            if locus in pa['loci'] and locus not in processed_loci:
                weights = get_weights_for_scores(locus, list(pa['loci'].keys()))
                weight_arrays.append(weights)
                processed_loci.add(locus)
        if weight_arrays:
            sum_pa = apply_weights(pa, weight_arrays)
            pa['gfw'] = key
            pa['sum'] = sum_pa
                    
        if sum_pa:
            max_sum = max(sum_pa.values())
            if max_sum != 0:
                pa['gfs'] = {win: val / max_sum for win, val in sum_pa.items()}
            else:
                pa['gfs'] = {win: 0 for win, val in sum_pa.items()}
            if write:
                write_gfs(pa, gator_scores_path, key, idx_win_map)
        if deduplication:
            deduplicated_wins, itself_comp = get_deduplicate_wins(gfs, pa['gfs'].items(), idx_win_map, deduplicated_wins, itself_comp, key, verbose)
    if deduplication:
        return gfs, deduplicated_wins, itself_comp
    else:
        return gfs
        
def get_weights_for_scores(
        locus: str,
        loci: List[str]
) -> np.ndarray:
    """
    Generates weight arrays for scores based on the position of the locus within a list of loci.

    Args:
        locus: The focal locus for which weights are to be calculated.
        loci: List of all loci.

    Returns:
        An array of scaled weights.
    """
    loci_count = len(loci) + 1
    num_loci_array = np.arange(1, loci_count)
    position_loci_in_window = loci.index(locus) + 1
    stan_dev = (loci_count * 15) / 200
    probability_function = norm.pdf(num_loci_array, loc=position_loci_in_window, scale=stan_dev)
    scaled_weights = probability_function / np.max(probability_function)
    return scaled_weights

def apply_weights(
        pa: Dict[str, Any],
        weight_arrays: List[np.ndarray]
) -> Dict[int, float]:
    """
    Applies calculated weights to the alignment data for each locus.

    Args:
        pa (Dict[str, Any]): Dictionary containing loci and their respective presence absence data.
        weight_arrays (List[np.ndarray]): List of weight arrays calculated for each locus.

    Returns:
        Dict[int, float]: A dictionary mapping each window to the sum of weighted scores.
    """
    weight_arrays = np.array(weight_arrays)
    max_scaled_weights = np.max(weight_arrays, axis=0)
    sum_pa = defaultdict(float)
    loci = list(pa['loci'].keys())
    max_index = len(max_scaled_weights)
    all_wins = np.stack([pa['loci'][locus] for locus in loci[:max_index]])
    weighted_wins = all_wins * max_scaled_weights[:, np.newaxis]
    sum_pa_array = np.sum(weighted_wins, axis=0)
    for i, locus in enumerate(loci[:max_index]):
        pa['loci'][locus] = weighted_wins[i]
    sum_pa = {win: val for win, val in enumerate(sum_pa_array)}
    return sum_pa

def write_gfs(gfs, output, focal_window, idx_win_map):
    """
    Writes gator focal scores (GFS) to a CSV file based on the provided data.

    Args:
        gfs (dict): Dictionary containing loci and their GFS data.
        output (str): Directory where the CSV file should be saved.
        focal_window (str): Identifier for the focal window, used to name the file.
        idx_win_map (dict): Mapping of indices to window identifiers for labeling rows.

    This function ensures the CSV file includes headers and sorted rows based on GFS.
    """
    os.makedirs(output, exist_ok=True)
    filename = f"{focal_window[:-5]}_GFS.csv"
    output_file = os.path.join(output, filename)
    loci_keys = list(gfs['loci'].keys())
    sum_keys = list(gfs['sum'].keys())
    try:
        with open(output_file, mode='w', newline='') as oh:
            writer = csv.writer(oh)
            header = ['Gator Windows/Focal Loci'] + loci_keys + ['Gator Focal Window', 'Gator Focal Score']
            writer.writerow(header)
            rows = []
            for i, win in enumerate(sum_keys):
                #if gfs['gfs'].get(win, 0) == 0:
                    #continue
                row = [idx_win_map.get(win, win)] + [gfs['loci'][locus][i] for locus in loci_keys] + [gfs['gfw'], gfs['gfs'][win]]
                rows.append((gfs['gfs'][win], row))
            sorted_rows = sorted(rows, key=itemgetter(0), reverse=True)
            for _, row in sorted_rows:
                writer.writerow(row)
    except Exception as e:
        logging.error(f"Failed to write GFS data to {output_file}: {e}")
        raise IOError(f"Failed to write to file {output_file}. Error: {e}")

def get_deduplicate_wins(
    gfs: Dict[str, Dict],
    items: List[Tuple[int, float]],
    idx_win_map: Dict[int, str],
    deduplicate_wins: Dict[str, str],
    itself_comp: Dict[str, str],
    focal: str,
    verbose: bool = False
) -> Tuple[Dict[str, str], Dict[str, str]]:
    """
    Filters and updates a dictionary with deduplicate windows based on gator focal score of 1, which means identical gator window.
    
    Args:
        gfs (Dict): Dictionary containing genomic feature scores data.
        items (List[Tuple[int, float]]): List of tuples with window indices and their corresponding scores.
        idx_win_map (Dict[int, str]): Mapping from indices to window identifiers.
        deduplicate_wins (Dict[str, str]): Dictionary to update with selected duplicate windows.
        itself_comp (Dict[str, str]): Dictionary tracking the values that are overwriting deduplicate_wins, 
                                      for window comparisons where the window and focal are identical.
        focal (str): Identifier of the current gator focal window to map to deduplicate windows.
        verbose (bool, optional): Flag to enable verbose logging. Default is False.
    
    Returns:
        Tuple[Dict[str, str], Dict[str, str]]: Updated dictionaries with selected subset windows mapped to their corresponding gator focal windows.
    
    This function iterates over provided score items and selects those that meet a predefined condition,
    such as a gator focal score of 1.0. The selected windows are then mapped to a given gator focal window, after stripping
    the '.gbff' genbank extension.
    
    The function also handles the scenario where a deduplicated window key already exists in the deduplicate_wins dictionary.
    In such cases, it updates the itself_comp dictionary to keep track of which focal window will be overwritten without this.
    This is particularly useful for comparing windows to themselves (self-comparisons).
    """
    try:
        for win, val in items:
            if val == 1.0:
                window_name = idx_win_map[win][:-5]
                if window_name in deduplicate_wins:
                    current_value = deduplicate_wins[window_name]
                    if verbose:
                        logging.debug(f"Found existing window gator key: {window_name} with current gator focal window: {current_value} as value. Storing new value in itself_comp.")
                    itself_comp[window_name] = focal[:-5]
                    if verbose:
                        logging.debug(f"Updated itself_comp for window gator key {window_name} with new value: {focal[:-5]}")
                else:
                    deduplicate_wins[window_name] = focal[:-5]
                    if verbose:
                        logging.debug(f"Added new window gator as key: {window_name} with value: the focal gator window {focal[:-5]}")
    except KeyError as e:
        logging.error(f"Key error in get_deduplicate_wins: {e}")
        raise KeyError(f"Failed due to missing gator focal window: {e}")
    return deduplicate_wins, itself_comp

def write_concat_gfs(
    conc_output: str,
    gfs_dict: Dict[str, Dict],
    idx_win_map: Dict[int, str],
    full_set: Set[str],
    n: int,
    deduplication: bool
) -> Union[None, Set[str]]:
    """
    Writes gator focal scores (GFS) to a CSV file, handling both deduplication and concatenation.

    Args:
        conc_output (str): Directory where the output files will be saved.
        gfs_dict (Dict): Dictionary containing the gator focal score data.
        idx_win_map (Dict[int, str]): Mapping from indices to window identifiers.
        full_set (Set[str]): Set to keep track of unique windows during deduplication.
        n (int): Current iteration or step number, used for deduplication logic.
        deduplication (bool): Flag indicating if deduplication is being performed.

    Returns:
        Union[None, Set[str]]: Returns the updated full_set if deduplication is True, otherwise None.
    """
    deduplication_scores = os.path.join(conc_output, 'deduplication_data')
    concatenated_scores = os.path.join(conc_output, 'concatenated_scores')
    if deduplication:
        os.makedirs(deduplication_scores, exist_ok=True)
        filepath = os.path.join(deduplication_scores, 'deduplication_gfs.csv')
    else:
        os.makedirs(concatenated_scores, exist_ok=True)
        filepath = os.path.join(concatenated_scores, 'concatenated_gfs.csv')
    file_empty = not os.path.exists(filepath) or os.stat(filepath).st_size == 0
    try:
        with open(filepath, mode='a', newline='') as oh:
            writer = csv.writer(oh)
            if file_empty:
                writer.writerow(['Gator Focal Window', 'Gator Window', 'Gator Focal Score'])
            for focal in gfs_dict:
                gfs = gfs_dict[focal]
                rows = []
                for win_index in gfs['sum']:
                    if deduplication: ## this is new
                        if gfs['gfs'][win_index] == 0:
                            continue
                    row = [gfs['gfw'], idx_win_map[win_index], gfs['gfs'][win_index]]
                    rows.append((gfs['gfs'][win_index], row))
                    if deduplication and n == 1:
                        full_set.add(idx_win_map[win_index][:-5])
                sorted_rows = sorted(rows, key=itemgetter(0), reverse=True)
                for _, row in sorted_rows:
                    writer.writerow(row)
    except Exception as e:
        logging.error(f"Failed to write GFS data to {filepath}: {e}")
        raise IOError(f"Failed to write to file {filepath}. Error: {e}")

    if deduplication:
        return full_set

def subset_dbfaa(
        db: str,
        deduplicate_wins: Dict[str, Any],
        output_faa: str,
        repre: str,
        sorted_windows: List[str],
        verbose: bool = False
) -> Optional[str]:
    """
    Creates a subset of the database by excluding sequences associated with duplicate windows.

    Args:
        db (str): Path to the input FASTA database file.
        deduplicate_wins (Dict[str, Any]): Dictionary of windows to exclude.
        output_faa (str): Path to the output FASTA file.
        repre (str): Identifier of the representative window to retain.
        sorted_windows (List[str]): List of sorted windows, updated to exclude the representative window.
        verbose (bool): If True, enables detailed logging information.

    Returns:
        Optional[str]: Path to the output FASTA file if sequences are written, None otherwise.
    """
    windows_to_remove = set(deduplicate_wins.keys())
    windows_to_remove.discard(repre)
    for win in windows_to_remove:
        if win in sorted_windows:
            sorted_windows.remove(win)
    sequences_to_write = []
    keep_sequence = False
    try:
        with open(db, 'r') as fh:
            for line in fh:
                if line.startswith('>'):
                    window = line.strip().split('|-|')[-1][:-5]
                    if window in windows_to_remove:
                        keep_sequence = False
                        if verbose:
                            logging.debug(f"Excluding sequence for window: {window}")
                        continue
                    keep_sequence = True
                    sequences_to_write.append(line)
                elif keep_sequence:
                    sequences_to_write.append(line)
    except Exception as e:
        logging.error(f"Error reading input database {db}: {e}")
        raise IOError(f"Failed to read input database {db}. Error: {e}")
    
    if sequences_to_write:
        try:
            with open(output_faa, 'w') as out_fh:
                out_fh.writelines(sequences_to_write)
            if verbose:
                logging.debug(f"Successfully wrote subset database to {output_faa}")
            return output_faa
        except Exception as e:
            logging.error(f"Error writing output database {output_faa}: {e}")
            raise IOError(f"Failed to write output database {output_faa}. Error: {e}")
    else:
        logging.warning(f"No sequences to write for the subset database: {output_faa}")
        return output_faa

def make_newdbfaa(
        faa: str,
        appended: str,
        verbose: bool = False
) -> None:
    """
    Appends the contents of one file to another.

    Args:
        faa (str): Path to the FASTA window  whose contents need to be appended.
        appended (str): Path to the target file where contents will be appended.
        verbose (bool): If True, enables detailed logging information.

    This function reads the entire content of `faa` and appends it to `appended`.
    """
    try:
        with open(faa, 'r') as file_to_append:
            content = file_to_append.read()
        try:
            with open(appended, 'a') as target_file:
                target_file.write(content)
                if verbose:
                    logging.debug(f"Successfully appended content to target file {appended}")
        except Exception as e:
            logging.error(f"Error writing to target file {appended}: {e}")
            raise IOError(f"Failed to write to target file {appended}. Error: {e}")
    except Exception as e:
        logging.error(f"Error reading from source file {faa}: {e}")
        raise IOError(f"Failed to read from source file {faa}. Error: {e}")

def calculating_gator_conservation_percentages(
        pa_table: Dict[str, Dict],
        deduplicate_windows: List[str],
        verbose: bool = False
) -> Dict[str, Dict[str, float]]:
    """                                                                                                                                                  
    Calculate the Gator Conservation Percentages from the presence-absence tables.                                                                       
                                                                                                                                                         
    Args:                                                                                                                                                
        pa_table (Dict[str, Dict]): A dictionary where keys are window identifiers and values are presence-absence data.                                 
        deduplicate)windows (List[str]): A list of deduplicate windows to process.                                                                       
        verbose (bool): If True, enables detailed logging information.                                                                                   
                                                                                                                                                         
    Returns:                                                                                                                                             
        Dict[str, Dict[str, float]]: A dictionary with conservation percentages, structured with outer keys as                                           
                                     Gator Focal Windows, inner keys as loci, and values as conservation percentages.                                    
    """
    percentages = {}
    try:
        for unq in deduplicate_windows:
            if unq in pa_table:
                pa_tables = pa_table[unq]
                window_genbank = os.path.splitext(os.path.basename(unq))[0]
                loci = pa_tables['loci'].keys()
                percentages[window_genbank] = {}
                valid_rows = []
                for i in range(len(next(iter(pa_tables['loci'].values())))):
                    if any(pa_tables['loci'][locus][i] != 0 for locus in loci):
                        valid_rows.append(i)
                for locus in loci:
                    values = [pa_tables['loci'][locus][i] for i in valid_rows]
                    if values:
                        percentage = sum(values) / len(values)
                        percentages[window_genbank][locus] = percentage
        return percentages
    except Exception as e:
        logging.error(f"Error calculating conservation percentages: {e}")
        raise ValueError(f"Failed to calculate conservation percentages. Error: {e}")

def generating_clustermap(
        concatenated_GFSs: str,
        output_dir: str,
        verbose: bool = False
) -> None:
    """
    Generates a clustermap for the GFSs.

    Args:
        concatenated_GFSs (str): Path to the dataframe containing the GFSs matrix.
        output_dir (str): Path to the directory where the clustermap will be saved.
        verbose (bool): If True, enables detailed logging information.

    Returns:
        None: This function generates and saves the clustermap.
    """
    try:
        gcfs = pd.read_csv(concatenated_GFSs, delimiter=",")
        gcfs['Gator Window'] = gcfs['Gator Window'].apply(lambda x: os.path.splitext(x)[0])
        gcfs['Gator Focal Window'] = gcfs['Gator Focal Window'].apply(lambda x: os.path.splitext(x)[0])
        matrix_gcfs = gcfs.pivot(index='Gator Window', columns='Gator Focal Window', values='Gator Focal Score').fillna(0)
        distances = pdist(matrix_gcfs, metric='euclidean')
        linkage_matrix = linkage(distances, method='complete')
        base_size = 10
        scaling_factor = len(matrix_gcfs) / 30
        proportional_size = max(base_size * scaling_factor, base_size)
        fig = plt.figure(figsize=(proportional_size, proportional_size))
        ax2 = fig.add_axes([0.028, 0.05, 0.24, 0.65])
        labels = matrix_gcfs.index.tolist()
        dend2 = dendrogram(linkage_matrix, labels=labels, link_color_func=lambda x: 'black', color_threshold=np.inf, orientation='left')
        ax2.set_xticks([])
        ax2.set_yticks([])
        ax2.axis('off')
        ax2.invert_yaxis()
        ax3 = fig.add_axes([0.27, 0.05, 0.68, 0.65])
        font_size = 35 / np.sqrt(len(matrix_gcfs))
        cax = sns.heatmap(matrix_gcfs.iloc[dend2['leaves'], dend2['leaves']],
                          ax=ax3,
                          linewidths=1,
                          linecolor='black',
                          cmap='viridis',
                          cbar_kws={'label': 'Gator Focal Score', 'shrink': 1},
                          xticklabels=False,
                          cbar=False,
                          annot_kws={"size": font_size}
                          )
        ax3.yaxis.tick_right()
        ax3.yaxis.set_label_position("right")
        ax3.set_yticklabels(ax3.get_yticklabels(), rotation=0)
        cbar_ax = fig.add_axes([0, 0.7, 0.02, 0.2])
        cbar = fig.colorbar(cax.collections[0], cax=cbar_ax)
        cbar.set_label('Gator Focal Scores')
        plt.savefig(output_dir, bbox_inches='tight')
        if verbose:
            logging.info(f"Clustered heatmap with gator focal scores saved to {output_dir}")
    except Exception as e:
        logging.error(f"Error generating clustermap: {e}")
        raise RuntimeError(f"Failed to generate clustermap. Error: {e}")

def opacity_to_hex(
        opacity: float
) -> str:
    """
    Converts an opacity value (between 0 and 1) to a two-character hexadecimal string.

    Args:
        opacity (float): Opacity value from 0 to 1.

    Returns:
        str: A two-character hexadecimal string representing the opacity.
    """
    if not (0 <= opacity <= 1):
        raise ValueError("Opacity must be between 0 and 1.")
    return hex(int(opacity * 255))[2:].zfill(2)
    
def get_label_figures(
        is_focal: str,
        gator_query: str,
        gator_hit: str,
        gator_nrps: bool,
        gator_pks: bool,
        annotation: str
) -> Optional[str]:
    """
    Generates a label for genomic features based on provided parameters.

    Args:
        is_focal (str): Indicator if the feature is focal.
        gator_query (str): Gator query type (e.g., 'req', 'opt').
        gator_hit (str): Gator hit information.
        gator_nrps (bool): Indicates if the feature is NRPS.
        gator_pks (bool): Indicates if the feature is PKS.
        annotation (str): Annotation for the feature.

    Returns:
        Optional[str]: The generated label or None if not focal.
    """
    if not is_focal:
        return
    if gator_hit:
        return f'[{gator_query}:{gator_hit}] {annotation}'
    else:
        if gator_nrps and gator_pks:
            return f'[{gator_query}:gator_nrps_pks] {annotation}'
        elif gator_nrps:
            return f'[{gator_query}:gator_nrps] {annotation}'
        else:
            return f'[{gator_query}:gator_pks] {annotation}'

def making_tracks(
        track: object,
        is_focal: str,
        loci_list: List[Tuple],
        percentages:
        Dict[str, Dict[str, float]]
) -> None:
    """
    Adds features to a genomic track visualization.

    Args:
        track (object): Track object to which features will be added.
        is_focal (str): Indicator if the features are focal.
        loci_list (List[Tuple]): List of tuples containing loci information.
        percentages (Dict[str, Dict[str, float]]): Dictionary containing conservation percentages for loci.

    Returns:
        None
    """
    for index, cds_tuple in enumerate(loci_list, 1):
        start, end, strand, locus, annotation, gator_query, gator_nrps, gator_pks, gator_hit, contig_edge = cds_tuple
        opacity = next((inner[locus] for inner in percentages.values() if locus in inner), 0)
        alpha = opacity_to_hex(opacity)
        if gator_query == 'req':
            color = '#7570B3'
            label = get_label_figures(is_focal, gator_query, gator_hit, gator_nrps, gator_pks, annotation)
        elif gator_query == 'opt':
            color = '#C87137' + alpha
            label = get_label_figures(is_focal, gator_query, gator_hit, gator_nrps, gator_pks, annotation)
        else:
            color = '#008423' + alpha
            label = ""
        try:
            track.add_feature(start, end, strand, facecolor="white")
            edgecolor = "red" if contig_edge else "black"
            track.add_feature(start, end, strand, label=label, facecolor=color, labelsize=15, linewidth=1.5, labelvpos="top", edgecolor=edgecolor)
        except Exception as e:                                                                                                                                                                       
            logging.error(f"Error adding the track feature for locus {locus}: {e}") 

def create_genome_viz() -> GenomeViz:
    """
    Creates a GenomeViz instance with predefined configuration settings.

    Returns:
        GenomeViz: An instance of the GenomeViz class configured with specific parameters.
    """
    try:
        genome_viz = GenomeViz(
            fig_width=20,
            fig_track_height=0.5,
            align_type="center",
            feature_track_ratio=2.0,
            link_track_ratio=3.0,
            tick_track_ratio=1.0,
            track_spines=False,
            tick_style="bar",
            plot_size_thr=0,
            tick_labelsize=15
        )
        return genome_viz
    except Exception as e:
        logging.error(f"Error creating GenomeViz instance: {e}")
        raise RuntimeError(f"Failed to create GenomeViz instance. Error: {e}")

def set_colorbar(
        gv: GenomeViz,
        fig: Any,
        num_windows: int,
        adaptable_height: bool = True
) -> None:
    """
    Sets the color bar for the genome visualization figure.

    Parameters:
    gv (GenomeViz): The genome visualization object.
    fig (Figure): The figure object to which the color bar will be added.
    num_windows (int): The number of windows, used for calculating adaptable height.
    adaptable_height (bool): If True, adjusts the height based on the number of windows.
    """
    bar_height = 1.4 / num_windows if adaptable_height else 1.4

    gv.set_colorbar(
        fig,
        bar_colors=['#7570B3ff', '#C87137ff', '#008423ff'],
        alpha=1,
        vmin=0,
        vmax=100,
        bar_height=bar_height,
        bar_label="Conservation",
        bar_labelsize=13
    )
    
def gator_conservation_plot(
        windows: Dict[str, Dict],
        percentages: Dict[str, Dict[str, float]],
        directory_output: str,
        unq_windows: List[str],
        adaptable_height: bool = False,
        verbose: bool = True
) -> None:
    """
    Generates a gator conservation figure for each window.

    Args:
        windows (Dict[str, Dict]): Dictionary containing window information including track name, record length, and meta loci.
        percentages (Dict[str, Dict[str, float]]): Dictionary with conservation percentages for each locus in each window.
        directory_output (str): Path to the directory where the conservation figures will be saved.
        unq_windows (List[str]): List of unique window identifiers to process.
        adaptable_height (bool): If True, adjusts the height of the color bar based on the number of windows.
        verbose (bool): If True, enables detailed logging.

    Returns:
        None: The function generates and saves conservation figures as SVG files.
    """
    try:
        for unq in unq_windows:
            if unq in windows:
                window_name = os.path.splitext(unq)[0]
                gv = create_genome_viz()
                window_data = windows[unq]
                track = gv.add_feature_track(
                    window_data['track_name'],
                    window_data['record_length'],
                    labelmargin=0.03,
                    linecolor="#333333",
                    linewidth=2
                )
                making_tracks(track, True, window_data['meta_loci'], percentages)
                track.set_sublabel(text=f"{round(window_data['record_length']/1000, 2)} Kb", ymargin=1.5)
                os.makedirs(directory_output, exist_ok=True)
                output_filepath = os.path.join(directory_output, f"{window_name}.svg")
                fig = gv.plotfig()
                set_colorbar(gv, fig, 1, adaptable_height)
                fig.savefig(output_filepath)
                fig.clf() # clear the figure from memory
                plt.close(fig) # close the plot to release memory 
                if verbose:
                    logging.debug(f"Generated conservation plot for window: {window_name}")
        if verbose:
            logging.info("All gator conservation figures have been generated successfully.")
    except Exception as e:
        logging.error(f"Error generating conservation plots: {e}")
        raise RuntimeError(f"Failed to generate conservation plots. Error: {e}")

def ordering_windows_by_GFSs(
        concatenated_GFSs: str
) -> Tuple[Dict[str, List[str]], Dict[str, Dict[str, float]]]:
    """
    Orders the windows based on the Gator focal scores for each Gator focal window.

    Args:
        concatenated_GFSs (str): Path to the concatenated table with information about Gator focal windows and Gator focal scores.

    Returns:
        Tuple[Dict[str, List[str]], Dict[str, Dict[str, float]]]: 
            - A dictionary with Gator focal windows as keys and lists of ordered Gator windows as values.
            - A dictionary with Gator focal windows as keys and dictionaries of Gator windows with their scores as values.
    """
    ordered_windows_w_focal = {}
    ordered_windows_GFS = {}
    try:
        with open(concatenated_GFSs, 'r') as fh:
            next(fh) 
            for row in fh:
                columns = row.strip().split(',')
                gator_focal, gator_no_focal, gator_score = columns[0], columns[1], float(columns[2])
                if gator_focal not in ordered_windows_w_focal:
                    ordered_windows_w_focal[gator_focal] = []
                ordered_windows_w_focal[gator_focal].append(gator_no_focal)
                if gator_no_focal == gator_focal:
                    ordered_windows_w_focal[gator_focal].remove(gator_no_focal)
                    ordered_windows_w_focal[gator_focal].insert(0, gator_no_focal)
                if gator_focal not in ordered_windows_GFS:
                    ordered_windows_GFS[gator_focal] = {}
                ordered_windows_GFS[gator_focal][gator_no_focal] = gator_score
        return ordered_windows_w_focal, ordered_windows_GFS
    except FileNotFoundError as e:
        logging.error(f"File not found: {concatenated_GFSs}")
        raise
    except Exception as e:
        logging.error(f"Error ordering windows by Gator focal scores: {e}")
        raise

def gather_focal_hits(
    unq: str,
    loci_list_focal: List[Tuple[int, int, int, str, str, str, bool, bool, str, bool]],
    all_hits: Dict[str, Dict[str, float]],
    verbose: bool = False
) -> Optional[Tuple[List[str], str]]:
    """
    Gathers focal hits from the provided loci list and all hits dictionary.

    Args:
        unq (str): A string for the focal window name.
        loci_list_focal (List[Tuple[int, int, int, str, str, str, bool, bool, str, bool]]): A list containing all the information about the proteins for the focal windows.
        all_hits (Dict[str, Dict[str, float]]): A dictionary containing all the protein diamond alignment hits in a nested dictionary for each protein.
        verbose (bool): If True, enables detailed logging.

    Returns:
        Optional[Tuple[List[str], str]]: A tuple containing a list of focal hits and the strand string for the focal window, or None if no hits are found.
    """
    try:
        for i, cds_focal in enumerate(loci_list_focal, 1):
            startf, endf, strandf, locusf, productf, gqueryf, gnrpsf, gpksf, ghitf, contig_edge = cds_focal
            if gqueryf == 'req':
                if not gnrpsf and not gpksf:  # Both are False
                    if locusf in all_hits:
                        hlog = list(all_hits[locusf].keys())
                        if verbose:
                            logging.debug(f"Found focal hits for locus {locusf} where both gnrpsf and gpksf are False: {hlog}")
                        return (hlog, strandf)
                elif not gnrpsf or not gpksf:  # Only one is False
                    if locusf in all_hits:
                        hlog = list(all_hits[locusf].keys())
                        if verbose:
                            logging.debug(f"Found focal hits for locus {locusf} where one of gnrpsf or gpksf is False: {hlog}")
                        return (hlog, strandf)
                elif gnrpsf and gpksf:  # Both are True
                    if locusf in all_hits:
                        hlog = list(all_hits[locusf].keys())
                        if verbose:
                            logging.debug(f"Found focal hits for locus {locusf} where both gnrpsf and gpksf are True: {hlog}")
                        return (hlog, strandf)
        if verbose:
            logging.info(f"No focal ({unq}) hits found.")
        return None
    except Exception as e:
        if verbose:
            logging.error("Error in gathering focal hits: " + str(e))
        return None

def flip_windows_genbanks(
        hlog: List[str],
        strandf: str,
        record_length: int,
        loci_list_no_focal: List[Tuple],
        window_name: str,
        verbose: bool = False
) -> Tuple[List[Tuple], bool]:
    """
    Updates the strand and loci positions for non-focal windows if the strand for the first focal required protein
    is different from the one in the non-focal window.

    Args:
        hlog (List[str]): A list containing focal hits.
        strandf (str): A string containing the strand for the focal window.
        record_length (int): The length of the genomic record.
        loci_list_no_focal (List[Tuple]): A list containing all the information about the proteins for the non-focal windows.
        verbose (bool): If True, enables detailed logging.

    Returns:
        Tuple[List[Tuple], bool]:
            - A list containing the updated information about the proteins for the non-focal windows.
            - A boolean indicating whether a flip was performed.
    """
    flip = False
    try:
        for i, cds_no_focal in enumerate(loci_list_no_focal, 1):
            startn, endn, strandn, locusn, productn, gqueryn, gnrpsn, gpksn, ghitn, contig_edge = cds_no_focal
            if locusn in hlog and strandf != strandn:
                flip = True
                break

        if flip:
            new_loci_list = []
            for i, cds_no_focal in enumerate(loci_list_no_focal, 1):
                startn, endn, strandn, locusn, productn, gqueryn, gnrpsn, gpksn, ghitn, contig_edge = cds_no_focal
                nstrand = strandn * -1
                nstartn = record_length - endn
                nendn = record_length - startn
                new_loci_list.append((nstartn, nendn, nstrand, locusn, productn, gqueryn, gnrpsn, gpksn, ghitn, contig_edge))
            if verbose:
                logging.debug(f"Performed flip on windows genbanks for window: {window_name}.")
            return new_loci_list, flip
        else:
            if verbose:
                logging.debug(f"No flip needed for windows genbanks for window: {window_name}.")
            return loci_list_no_focal, flip
    except Exception as e:
        logging.error(f"Error flipping windows genbanks: {e}")
        raise RuntimeError(f"Failed to flip windows genbanks. Error: {e}")

def map_aa_gene(
        aa_start: int,
        aa_end: int,
        gene_start: int,
        gene_end: int,
        strand: int,
        verbose: bool = False
) -> Tuple[int, int]:
    """
    Maps amino acid positions to nucleotide positions within a gene, considering the strand.

    Args:
        aa_start (int): Start position of the amino acid.
        aa_end (int): End position of the amino acid.
        gene_start (int): Start position of the gene in nucleotide sequence.
        gene_end (int): End position of the gene in nucleotide sequence.
        strand (int): Strand of the gene (1 for forward, -1 for reverse).
        verbose (bool): If True, enables detailed logging.

    Returns:
        Tuple[int, int]: Mapped start and end positions in the gene's nucleotide sequence.
    """
    try:
        aa_length = aa_end - aa_start + 1
        if strand == 1:
            gene_aa_start = gene_start + (aa_start - 1) * 3
            gene_aa_end = gene_aa_start + aa_length * 3 - 1
        elif strand == -1:
            gene_aa_end = gene_end - (aa_start - 1) * 3
            gene_aa_start = gene_aa_end - aa_length * 3 + 1
        else:
            raise ValueError("Strand must be 1 or -1.")
        
        gene_aa_start = max(0, gene_aa_start)        
        if verbose:
            logging.debug(f"Mapped AA positions {aa_start}-{aa_end} to nucleotide positions {gene_aa_start}-{gene_aa_end} on strand {strand}.")
        return gene_aa_start, gene_aa_end
    
    except Exception as e:
        logging.error(f"Error mapping amino acid to gene: {e}")
        raise ValueError(f"Failed to map amino acid to gene. Error: {e}")

def prepare_window_to_genomeviz(
    unq: str,
    windows: Dict[str, Dict],
    ordered_windows_w_focal: Dict[str, List[str]],
    windows_identity: Dict[str, Dict],
    directory_output: str,
    percentages: Dict[str, Dict[str, float]],
    ordering_windows_by_GFS: Dict[str, List[str]],
    unq_windows: List[str],
    all_hits: Dict[str, Dict],
    adaptable_height: bool = True,
    verbose: bool = False
) -> None:
    """
    Prepares and visualizes genomic data using GenomeViz, generating a conservation plot for specified windows.

    Args:
        unq (str): Unique window identifier.
        windows (Dict[str, Dict]): Dictionary containing window data.
        ordered_windows_w_focal (Dict[str, List[str]]): Dictionary of ordered windows based on focal scores.
        windows_identity (Dict[str, Dict]): Dictionary containing window identity data.
        directory_output (str): Path to the output directory.
        percentages (Dict[str, Dict[str, float]]): Dictionary with conservation percentages for each locus.
        ordering_windows_by_GFS (Dict[str, List[str]]): Dictionary for ordering windows based on GFS.
        unq_windows (List[str]): List of unique window identifiers.
        all_hits (Dict[str, Dict]): Dictionary containing all the protein diamond alignment hits.
        adaptable_height (bool): If True, adjusts the height of the color bar based on the number of windows.                                                                                                     
        verbose (bool): If True, enables detailed logging.

    Returns:
        None: The function generates and saves conservation plots as SVG files.
    """
    try:
        window_dict = windows[unq]
        gv = create_genome_viz()
        hlog, strandf = gather_focal_hits(unq, window_dict["meta_loci"], all_hits, verbose)
        track = gv.add_feature_track(window_dict["track_name"], window_dict["record_length"], labelmargin=0.03, linecolor="#333333", linewidth=2)
        making_tracks(track, True, window_dict["meta_loci"], percentages)
        flips = ['False']
        
        # Process non-focal windows
        for window_name in ordering_windows_by_GFS[unq]:
            if window_name != unq:
                new_loci_list, flip = flip_windows_genbanks(hlog, strandf, windows[window_name]["record_length"], windows[window_name]["meta_loci"], window_name, verbose)
                flips.append(flip)
                track2 = gv.add_feature_track(windows[window_name]["track_name"], windows[window_name]["record_length"], labelmargin=0.03, linecolor="#333333", linewidth=2)
                if verbose:
                    logging.debug(f"Processed non-focal window {window_name}: flip={flip}")
                making_tracks(track2, False, new_loci_list, percentages)
        
        add_links_to_gv(gv, ordered_windows_w_focal, windows_identity, unq, flips)
        os.makedirs(directory_output, exist_ok=True)
        output_filepath = os.path.join(directory_output, f'{unq[:-5]}_neighborhoods.svg')
        fig = gv.plotfig()
        if verbose:
            logging.debug(f"Saved gator neighborhood plot to {output_filepath}")

        set_colorbar(gv, fig, len(windows), adaptable_height)
        fig.savefig(output_filepath)
        fig.clf() # clear the figure from memory
        plt.close(fig) # close the plot to release memory
    except Exception as e:
        logging.error(f"Error preparing window to GenomeViz: {e}")
        raise RuntimeError(f"Failed to prepare window to GenomeViz. Error: {e}")

def add_links_to_gv(
    gv: object,
    ordered_windows_w_focal: Dict[str, List[str]],
    windows_identity: Dict[str, Dict[str, Any]],
    unq: str,
    flips: List[bool],
    verbose: bool = False
) -> None:
    """
    Adds links to the GenomeViz visualization.

    Args:
        gv (object): GenomeViz instance.
        ordered_windows_w_focal (Dict[str, List[str]]): Dictionary of ordered windows based on focal scores.
        windows_identity (Dict[str, Dict]): Dictionary containing window identity data.
        unq (str): Unique window identifier.
        flips (List[bool]): List of boolean values indicating if the windows were flipped.
        verbose (bool): If True, enables detailed logging.

    Returns:
        None: The function adds links to the GenomeViz instance.
    """
    normal_color, inverted_color, alpha = "grey", "green", 0.5
    try:
        for i in range(len(ordered_windows_w_focal[unq]) - 1):
            max_bitscore_items = get_max_bitscore_items(ordered_windows_w_focal, windows_identity, unq, i)
            for locus_query, item in max_bitscore_items.items():
                create_links_to_gv(gv, item, i, flips)
                if verbose:
                    logging.debug(f"Added link for {locus_query} between {ordered_windows_w_focal[unq][i]} and {ordered_windows_w_focal[unq][i+1]}")
    except Exception as e:
        logging.error(f"Error adding links to GenomeViz for {unq}: {e}")
        raise RuntimeError(f"Failed to add links to GenomeViz for {unq}. Error: {e}")

def get_max_bitscore_items(
    ordered_windows_w_focal: Dict[str, List[str]],
    windows_identity: Dict[str, Dict[str, Any]],
    unq: str,
    i: int,
    verbose: bool = False
) -> Dict[str, Dict[str, Any]]:
    """
    Retrieves the items with the maximum bit scores for a given window.

    Args:
        ordered_windows_w_focal (Dict[str, List[str]]): Dictionary of ordered windows based on focal scores.
        windows_identity (Dict[str, Dict[str, Any]]): Dictionary containing window identity data.
        unq (str): Unique window identifier.
        i (int): Index of the window in the ordered list.
        verbose (bool): If True, enables detailed logging.

    Returns:
        Dict[str, Dict[str, Any]]: Dictionary of items with the maximum bit scores.
    """
    max_bitscore_items = {}
    try:
        current_window = ordered_windows_w_focal[unq][i]
        next_window = ordered_windows_w_focal[unq][i + 1]
        if next_window not in windows_identity[current_window]:
            if verbose:
                logging.debug(f"No homologous genes found between {current_window} and {next_window} based on the -idt and -qc parameters")
            return max_bitscore_items

        for item in windows_identity[current_window][next_window]:
            locus_query = item['locus_query']
            if item["query_nrps"] or item["query_pks"]:
                if locus_query not in max_bitscore_items or item['bit_score'] > max_bitscore_items[locus_query]['bit_score']:
                    max_bitscore_items[locus_query] = item
            else:
                max_bitscore_items[locus_query] = item
        if verbose:
            logging.debug(f"Max bitscore items for {unq} between {current_window} and {next_window}: {max_bitscore_items}")
        return max_bitscore_items
    except Exception as e:
        logging.error(f"Error retrieving max bitscore items for {unq}: {e}")
        raise RuntimeError(f"Failed to retrieve max bitscore items for {unq}. Error: {e}")

def create_links_to_gv(
    gv: object,
    item: Dict[str, Any],
    i: int,
    flips: List[bool],
    verbose: bool = False
) -> None:
    """
    Creates links in the GenomeViz visualization.

    Args:
        gv (object): GenomeViz instance.
        item (Dict[str, Any]): Item containing information to create the link.
        i (int): Index of the window in the ordered list.
        flips (List[bool]): List of boolean values indicating if the windows were flipped.
        verbose (bool): If True, enables verbose logging.

    Returns:
        None: The function adds links to the GenomeViz instance.
    """
    try:
        query_gene_aa_start, query_gene_aa_end = map_aa_gene(
            item['query_aa_start'], item['query_aa_end'], 
            item['query_start'], item['query_end'], 
            item['query_strand']
        )
        hit_gene_aa_start, hit_gene_aa_end = map_aa_gene(
            item['hit_aa_start'], item['hit_aa_end'], 
            item['hit_start'], item['hit_end'], 
            item['hit_strand']
        )
        link_data1 = (item['query_window'], query_gene_aa_start, query_gene_aa_end)
        link_data2 = (item['hit_window'], hit_gene_aa_start, hit_gene_aa_end)
        if i >= len(flips) - 1:
            return
        if flips[i] and i != 0:
            link_data1 = (
                item['query_window'], 
                (item['record_length_query'] - query_gene_aa_end), 
                (item['record_length_query'] - query_gene_aa_start)
            )
        if flips[i + 1]:
            link_data2 = (
                item['hit_window'], 
                (item['record_length_hit'] - hit_gene_aa_end), 
                (item['record_length_hit'] - hit_gene_aa_start)
            )
        gv.add_link(link_data1, link_data2, "grey", "green", 0.5, curve=True)
        if verbose:
            logging.debug(f"Created link in GenomeViz for {item['query_window']} and {item['hit_window']}")
    except Exception as e:
        logging.error(f"Error creating link in GenomeViz for {item['query_window']} and {item['hit_window']}: {e}")
        raise RuntimeError(f"Failed to create link in GenomeViz for {item['query_window']} and {item['hit_window']}. Error: {e}")

def make_gator_windows_neighborhood_figures(
    windows: Dict[str, Any],
    ordered_windows_w_focal: Dict[str, List[str]],
    windows_identity: Dict[str, Any],
    directory_output: str,
    percentages: Dict[str, Dict[str, float]],
    ordering_windows_by_GFS: Dict[str, Any],
    unq_windows: List[str],
    all_hits: Dict[str, Any],
    adaptable_height: bool =True,
    verbose: bool = False
) -> None:
    """
    Generates neighborhood figures for gator windows.

    Args:
        windows (Dict[str, Any]): Dictionary containing window data.
        ordered_windows_w_focal (Dict[str, List[str]]): Dictionary of ordered windows based on focal scores.
        windows_identity (Dict[str, Any]): Dictionary containing window identity data.
        directory_output (str): Path to the output directory.
        percentages (Dict[str, Dict[str, float]]): Dictionary with conservation percentages for each locus.
        ordering_windows_by_GFS (Dict[str, Any]): Dictionary for ordering windows based on GFS.
        unq_windows (List[str]): List of unique window identifiers.
        all_hits (Dict[str, Any]): Dictionary containing all the protein diamond alignment hits.
        adaptable_height (bool): If True, adjusts the height based on the number of windows.                                                                                                                      
        verbose (bool): If True, enables verbose logging.
    Returns:
        None: The function generates and saves neighborhood figures.
    """
    try:
        for unq in unq_windows:
            if unq in windows:
                prepare_window_to_genomeviz(
                    unq,
                    windows,
                    ordered_windows_w_focal, 
                    windows_identity,
                    directory_output, 
                    percentages,
                    ordering_windows_by_GFS,
                    unq_windows,
                    all_hits,
                    adaptable_height=True,
                    verbose=True
                )
        if verbose:
            logging.info("All gator neighborhood figures have been generated successfully.")
    except Exception as e:
        logging.error(f"Error generating neighborhood figures: {e}")
        raise RuntimeError(f"Failed to generate neighborhood figures. Error: {e}")

#################################################################################
#################################################################################
#################################################################################
#################################################################################
#################################################################################
############################# BEGIN MAIN BLOCK ##################################  

if __name__ == "__main__":
    ## Parse arguments
    args = parse_gator_arguments()

    ## print description
    print(DESCRIPTION)
    
    ## Make output directory
    create_directory(
        args.o,
        verbose=args.v
    )
    
    ## getting the genomes list
    genomes_list = get_list_genomes(
        args.g,
        verbose=args.v
    )

    ##defining files/folders outputs
    input_faas_path = os.path.join(args.o, 'faas')
    output_genbanks = os.path.join(args.o, 'windows_genbanks')
    output_table_hits = os.path.join(output_genbanks, 'gator_hits.tsv')
    output_pa_tables = os.path.join(args.o, 'presence_absence')
    output_gfs = os.path.join(args.o, 'gator_scores')
    output_concatenated = os.path.join(args.o, 'concatenated_scores')
    output_concatenated_gfs = os.path.join(output_concatenated, 'concatenated_gfs.csv')
    output_clustermap = os.path.join(output_concatenated, 'clustermap_gfs.svg')
    output_gator_conser_figs = os.path.join(args.o, 'gator_conservation_plots')
    output_gator_neigh_figs = os.path.join(args.o, 'gator_neighborhoods_plots')

    ## Initialize query protein dictionary
    q = {}
    q = parse_query_faa(
        args.rq,
        'req',
        q
    )

    if args.op is not None:
        q = parse_query_faa(
            args.op,
            'opt',
            q
        )

    ## Create combined required and optional queries fasta
    all_merged_queries = tempfile.NamedTemporaryFile(dir=args.o, prefix='all_merged_queries_', suffix='.faa', mode='w', delete=False)
    for header in q:
        all_merged_queries.write('>'+header+"\n"+q[header]['seq']+"\n")
    all_merged_queries.close()
    
    ## hmmsearch against PKS/NRPS modular domains and parse results
    with tempfile.NamedTemporaryFile(dir=args.o, prefix='all_merged_queries_', suffix='.domtbl', mode='w', delete=False) as query_modular_domtbl:
        run_hmmsearch(
            MODULAR_DOMAINS_HMMDB,
            all_merged_queries.name,
            query_modular_domtbl.name,
            args.t,
            args.e,
            verbose = args.v
        )
        modular_domain_hit = parse_modular_domtblout(
            query_modular_domtbl.name,
            verbose=args.v
        )
        parse_modular_domain_query_hits(
            modular_domain_hit,
            q,
            verbose=args.v
        )

    ## Create nonmodular fasta (to be subsequent diamond query)
    nonmodular_queries = tempfile.NamedTemporaryFile(dir=args.o, prefix='non_modular_queries_', suffix='.faa', mode='w', delete=False)
    for header in q:
        if not q[header]['is_nrps'] and not q[header]['is_pks']:
            nonmodular_queries.write('>'+header+"\n"+q[header]['seq']+"\n")
    nonmodular_queries.close()

    ## Getting paths for gator databases
    dmnd_database, domtblout_database = get_dmnd_domtblout_paths(
        args.d,
        verbose=args.v
    )

    ## parse genome modular search
    modular_domain_hit = parse_modular_domtblout(domtblout_database, verbose=args.v)
    q = parse_modular_domain_genome_hits(
        modular_domain_hit,
        q,
        verbose=args.v
    )

    # diamond against nonmodular fasta
    with tempfile.NamedTemporaryFile(dir=args.o, prefix='dmnd_out_', suffix='.txt', mode='w', delete=False) as dmnd_out:
        logging.info(f"Running Diamond with {nonmodular_queries.name} against {dmnd_database}")
        run_diamond(
            nonmodular_queries.name,
            dmnd_database,
            dmnd_out.name,
            args.t,
            args.qc,
            args.idt,
            args.us,
            args.bs,
            args.k,
            verbose=args.v
        )

    ## parse diamond search
    parse_diamond_search(
        dmnd_out.name,
        q,
        verbose=args.v
    )

    ## grouping query proteins hits by contig
    req_hits_by_contig, all_required_proteins = grouping_user_req_opt_proteins_by_contig(
        q,
        'req',
        verbose=args.v
    )
    opt_hits_by_contig, all_optional_proteins = grouping_user_req_opt_proteins_by_contig(
        q,
        'opt',
        verbose=args.v
    )

    ## making req and opt lists
    req_opt_locus, req_opt_hits = make_lists_req_opt(
        req_hits_by_contig,
        opt_hits_by_contig,
        verbose=args.v
    )

    ## checking distance between loci and presence of all required user proteins
    final_window = checking_distance_bw_loci(
        req_hits_by_contig,
        all_required_proteins,
        args.rd,
        verbose=args.v
    )

    ## generating windows Genbank files and protein database                                                                                                                                 
    with tempfile.NamedTemporaryFile(dir=args.o, prefix='allvall_proteins_', suffix='.faa', mode='w', delete=False) as dbfaa:
        map_pa, map_wc_wi, map_ge, map_dmnd, map_lo_wn, map_win_size = process_windows(
            input_faas_path,
            final_window,
            genomes_list,
            output_genbanks,
            WINDOW_EXTENSION_FACTOR,
            dbfaa,
            req_hits_by_contig,
            opt_hits_by_contig,
            verbose=args.v
        )

    ## write table hits
    prepare_hits_to_write(
        req_opt_hits,
        map_lo_wn,
        output_table_hits,
        verbose=args.v
    )
    
    ## creating the dmnd db for allvall                                                                                                                                                      
    with tempfile.NamedTemporaryFile(dir=args.o, prefix='dmnd_db_', suffix='.dmnd', mode='w', delete=False) as dmnd_db:
        create_diamond_database(
            dbfaa.name,
            dmnd_db.name,
            False,
            args.t,
            verbose=args.v
        )

    ## deduplication process and generation of dbfaa with unique windows
    with tempfile.NamedTemporaryFile(dir=args.o, prefix='deduplicate_allvall_proteins_', suffix='.faa', mode='w', delete=False) as newdbfaa:
        unq_windows = deduplication_for_windows(
            args.o,
            dbfaa.name,
            dmnd_db.name,
            map_pa,
            req_opt_locus,
            output_pa_tables,
            newdbfaa.name,
            map_dmnd,
            map_win_size,
            verbose=args.v
        )

    ## creating dmnd db for deduplicate dbfaa                                                                                                                                                
    with tempfile.NamedTemporaryFile(dir=args.o, prefix='deduplicate_dmnd_db_', suffix='.dmnd', mode='w', delete=False) as newdmnd_db:
        create_diamond_database(
            newdbfaa.name,
            newdmnd_db.name,
            False,
            args.t,
            verbose=args.v
        )

    ### remove redundant windows from map_pa                                                                
    for rwin in list(map_pa.keys()):
        if rwin not in unq_windows:
            map_pa.pop(rwin)
    
    ## running diamond for deduplicate data
    pa = {}
    with tempfile.NamedTemporaryFile(dir=args.o, prefix='deduplicate_dmnd_out_', suffix='.dmnd', mode='w', delete=False) as deduplicate_dmnd_out:
        run_diamond(
            newdbfaa.name,
            newdmnd_db.name,
            deduplicate_dmnd_out.name,
            args.t,
            args.qc,
            args.idt,
            False,
            args.bs,
            args.k,
            verbose=args.v
        )
        pa, idx_win_map, windows_identity, all_hits = parse_diamond_pa(
            deduplicate_dmnd_out.name,
            pa,
            map_pa,
            output_pa_tables,
            True,
            True,
            map_dmnd,
            verbose=args.v
        )

    ## presence absence tables are created at
    logging.info(f"Gene level presence absence tables for deduplicated gator windows created sucessfully in {output_pa_tables}")
    
    ## calculating locus conservation percentage                                                                                                                                         
    percentages = calculating_gator_conservation_percentages(
        pa,
        unq_windows,
        verbose=args.v
    ) 

    ## getting gator focal scores
    unq_gfs, itself_comp = {}, {}
    s_gfs = get_gfs(
        pa,
        req_opt_locus,
        args.o,
        idx_win_map,
        unq_gfs,
        itself_comp,
        False,
        True,
        verbose=args.v
    )

    ## gator focal scores tables are created at
    logging.info(f"Gator focal scores tables for deduplicated gator windows created sucessfully in {output_gfs}")

    ## write concatenated file with all gator focal scores
    empty_str = ''
    write_concat_gfs(
        args.o, s_gfs,
        idx_win_map,
        empty_str,
        empty_str,
        False
    )

    logging.info(f"Concatenated of gator focal scores across deduplicated gator windows created sucessfully in {output_concatenated_gfs}")

    ## making the clustermap for the gator focal scores concatenate
    if len(final_window) != 1 and len(unq_windows) != 1:
        generating_clustermap(
            output_concatenated_gfs,
            output_clustermap,
            verbose=args.v
        )

    ## making the gator conservation figures
    if args.nc is not True:
        gator_conservation_plot(
            map_ge,
            percentages,
            output_gator_conser_figs,
            unq_windows,
            adaptable_height=False,
            verbose=args.v
        )

    ## ordering windows based on gator focal scores
    if len(final_window) != 1 and len(unq_windows) != 1 and args.nn is not True:
        ordered_windows_w_focal, ordering_windows_by_GFS = ordering_windows_by_GFSs(output_concatenated_gfs)
        
    ## making the window neighborhood figures
    if len(final_window) != 1 and len(unq_windows) != 1 and args.nn is not True:
        make_gator_windows_neighborhood_figures(
            map_ge,
            ordered_windows_w_focal,
            windows_identity,
            output_gator_neigh_figs,
            percentages,
            ordering_windows_by_GFS,
            unq_windows,
            all_hits,
            adaptable_height=True,
            verbose=args.v
        )
    
    ## elapsed time
    elapsed_time(stime)

